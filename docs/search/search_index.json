{"config":{"indexing":"full","lang":["pt"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Amora Data Build Tool (ADBT) \u00b6 Amora Data Build Tool enables analysts and engineers to transform data on the data warehouse (BigQuery) by writing Amora Models that describe the data schema using Python's PEP484 - Type Hints and select statements with SQLAlchemy . Amora is able to transform Python code into SQL data transformation jobs that run inside the warehouse. When should I use? \u00b6 to-do Installation \u00b6 pip install amora Setup Big Query permissions \u00b6 In order to use Amora with BigQuery, you'll need to setup a keyfile . Go to the BigQuery credential wizard . Ensure that the right project is selected in the header bar. Generate credentials with the following options: Which API are you using? BigQuery API What data will you be accessing? Application data (you'll be creating a service account) Are you planning to use this API with App Engine or Compute Engine? No Service account name: amora-user Role : BigQuery Job User & BigQuery User Key type : JSON Download the JSON file and save it in an easy-to-remember spot, with a clear filename (e.g. ~/.bq-service-accounts/amora-user-credentials.json ) Set the environment variable GOOGLE_APPLICATION_CREDENTIALS to the path of the JSON file that contains your service account key. You can add export GOOGLE_APPLICATION_CREDENTIALS=~/.bq-service-accounts/amora-user-credentials.json to your shell initialization script ( .zshrc for zsh, .bash_profile for bash, ...) Required roles: BigQuery Data Editor ( roles/bigquery.dataEditor ) BigQuery User ( roles/bigquery.user ) How to use? \u00b6 Amora is packed with a compiler and a runner . The compiler (CLI amora compile ) compiles Amora's python models into SQL statements which can be executed against the configured data warehouse using amora materialize . The animation above displays an user inside an Amora Project using Amora's CLI amora compile to compile model files into SQL statements and creating the corresponding views/tables into the Data Warehouse (BigQuery) using amora materialize . An Amora project is a directory with .py files, each containing a model definition, a subclass of amora.models.AmoraModel . Models \u00b6 Models express both a data schema and a transformation statement: # project/models/heart_rate.py from datetime import datetime from amora.types import Compilable from amora.models import ( AmoraModel , ModelConfig , PartitionConfig , MaterializationTypes , select , Field ) from project.models.health import Health class HeartRate ( AmoraModel , table = True ): # model configuration __tablename__ = \"heart_rate\" __depends_on__ = [ Health ] __model_config__ = ModelConfig ( materialized = MaterializationTypes . table , partition_by = PartitionConfig ( field = \"creationDate\" , data_type = \"TIMESTAMP\" , granularity = \"day\" ), cluster_by = [ \"sourceName\" ], labels = { \"freshness\" : \"daily\" }, ) # data schema creationDate : datetime device : str endDate : datetime id : int = Field ( primary_key = True ) sourceName : str startDate : datetime unit : str value : float # transformation statement @classmethod def source ( cls ) -> Compilable : return select ( [ Health . creationDate , Health . device , Health . endDate , Health . id , Health . sourceName , Health . startDate , Health . unit , Health . value , ] ) . where ( Health . type == \"HeartRate\" ) HeartRate is an Amora Model that is materialized as a partitioned table named heart_rate , with data clustered by the sourceName column. The source of its data is a filter in Health model. Let's go through each part of the HeartRate model: __tablename__: str : Used to override the automatically generated name __depends_on__: List[AmoraModel] : A list of Amora Models that the current model depends on __model_config__: amora.models.ModelConfig materialized: amora.models.MaterializationTypes : The materialization configuration: view , table , ephemeral . Default: view partition_by: amora.models.PartitionConfig : BigQuery supports the use of a partition by clause to easily partition a table by a column or expression. This option can help decrease latency and cost when querying large tables. cluster_by: Union[str, List[str]] : BigQuery tables can be clustered to colocate related data. Expects a column of a list of columns. tags: List[str] : A list of tags that can be used as the resource selection source(cls) -> Compilable : The SELECT statement that defines the model resulting dataset","title":"Amora"},{"location":"#amora-data-build-tool-adbt","text":"Amora Data Build Tool enables analysts and engineers to transform data on the data warehouse (BigQuery) by writing Amora Models that describe the data schema using Python's PEP484 - Type Hints and select statements with SQLAlchemy . Amora is able to transform Python code into SQL data transformation jobs that run inside the warehouse.","title":"Amora Data Build Tool (ADBT)"},{"location":"#when-should-i-use","text":"to-do","title":"When should I use?"},{"location":"#installation","text":"pip install amora","title":"Installation"},{"location":"#setup-big-query-permissions","text":"In order to use Amora with BigQuery, you'll need to setup a keyfile . Go to the BigQuery credential wizard . Ensure that the right project is selected in the header bar. Generate credentials with the following options: Which API are you using? BigQuery API What data will you be accessing? Application data (you'll be creating a service account) Are you planning to use this API with App Engine or Compute Engine? No Service account name: amora-user Role : BigQuery Job User & BigQuery User Key type : JSON Download the JSON file and save it in an easy-to-remember spot, with a clear filename (e.g. ~/.bq-service-accounts/amora-user-credentials.json ) Set the environment variable GOOGLE_APPLICATION_CREDENTIALS to the path of the JSON file that contains your service account key. You can add export GOOGLE_APPLICATION_CREDENTIALS=~/.bq-service-accounts/amora-user-credentials.json to your shell initialization script ( .zshrc for zsh, .bash_profile for bash, ...) Required roles: BigQuery Data Editor ( roles/bigquery.dataEditor ) BigQuery User ( roles/bigquery.user )","title":"Setup Big Query permissions"},{"location":"#how-to-use","text":"Amora is packed with a compiler and a runner . The compiler (CLI amora compile ) compiles Amora's python models into SQL statements which can be executed against the configured data warehouse using amora materialize . The animation above displays an user inside an Amora Project using Amora's CLI amora compile to compile model files into SQL statements and creating the corresponding views/tables into the Data Warehouse (BigQuery) using amora materialize . An Amora project is a directory with .py files, each containing a model definition, a subclass of amora.models.AmoraModel .","title":"How to use?"},{"location":"#models","text":"Models express both a data schema and a transformation statement: # project/models/heart_rate.py from datetime import datetime from amora.types import Compilable from amora.models import ( AmoraModel , ModelConfig , PartitionConfig , MaterializationTypes , select , Field ) from project.models.health import Health class HeartRate ( AmoraModel , table = True ): # model configuration __tablename__ = \"heart_rate\" __depends_on__ = [ Health ] __model_config__ = ModelConfig ( materialized = MaterializationTypes . table , partition_by = PartitionConfig ( field = \"creationDate\" , data_type = \"TIMESTAMP\" , granularity = \"day\" ), cluster_by = [ \"sourceName\" ], labels = { \"freshness\" : \"daily\" }, ) # data schema creationDate : datetime device : str endDate : datetime id : int = Field ( primary_key = True ) sourceName : str startDate : datetime unit : str value : float # transformation statement @classmethod def source ( cls ) -> Compilable : return select ( [ Health . creationDate , Health . device , Health . endDate , Health . id , Health . sourceName , Health . startDate , Health . unit , Health . value , ] ) . where ( Health . type == \"HeartRate\" ) HeartRate is an Amora Model that is materialized as a partitioned table named heart_rate , with data clustered by the sourceName column. The source of its data is a filter in Health model. Let's go through each part of the HeartRate model: __tablename__: str : Used to override the automatically generated name __depends_on__: List[AmoraModel] : A list of Amora Models that the current model depends on __model_config__: amora.models.ModelConfig materialized: amora.models.MaterializationTypes : The materialization configuration: view , table , ephemeral . Default: view partition_by: amora.models.PartitionConfig : BigQuery supports the use of a partition by clause to easily partition a table by a column or expression. This option can help decrease latency and cost when querying large tables. cluster_by: Union[str, List[str]] : BigQuery tables can be clustered to colocate related data. Expects a column of a list of columns. tags: List[str] : A list of tags that can be used as the resource selection source(cls) -> Compilable : The SELECT statement that defines the model resulting dataset","title":"Models"},{"location":"cli/","text":"CLI \u00b6 amora compile \u00b6 Generates executable SQL from model files. Compiled SQL files are written to the ./target directory. Source code in amora/cli.py @app . command () def compile ( models : Optional [ Models ] = models_option , target : Optional [ str ] = target_option , ) -> None : \"\"\" Generates executable SQL from model files. Compiled SQL files are written to the `./target` directory. \"\"\" for model , model_file_path in list_models (): if models and model_file_path . stem not in models : continue source_sql_statement = model . source () if source_sql_statement is None : typer . echo ( f \"\u23ed Skipping compilation of model ` { model_file_path } `\" ) continue target_file_path = model . target_path ( model_file_path ) typer . echo ( f \"\ud83c\udfd7 Compiling model ` { model_file_path } ` -> ` { target_file_path } `\" ) content = compile_statement ( source_sql_statement ) target_file_path . parent . mkdir ( parents = True , exist_ok = True ) target_file_path . write_text ( content ) amora materialize \u00b6 Executes the compiled SQL against the current target database. Source code in amora/cli.py @app . command () def materialize ( models : Optional [ Models ] = models_option , target : str = target_option , draw_dag : bool = typer . Option ( False , \"--draw-dag\" ), ) -> None : \"\"\" Executes the compiled SQL against the current target database. \"\"\" model_to_task = {} for target_file_path in list_target_files (): if models and target_file_path . stem not in models : continue task = materialization . Task . for_target ( target_file_path ) model_to_task [ task . model . unique_name ] = task dag = materialization . DependencyDAG . from_tasks ( tasks = model_to_task . values ()) if draw_dag : dag . draw () for model in dag : try : task = model_to_task [ model ] except KeyError : typer . echo ( f \"\u26a0\ufe0f Skipping ` { model } `\" ) continue else : table = materialization . materialize ( sql = task . sql_stmt , model = task . model ) if table is None : continue typer . echo ( f \"\u2705 Created ` { model } ` as ` { table . full_table_id } `\" ) typer . echo ( f \" Rows: { table . num_rows } \" ) typer . echo ( f \" Bytes: { table . num_bytes } \" ) amora test \u00b6 Runs tests on data in deployed models. Run this after amora materialize to ensure that the date state is up-to-date. Source code in amora/cli.py @app . command () def test ( models : Optional [ Models ] = models_option , ) -> None : \"\"\" Runs tests on data in deployed models. Run this after `amora materialize` to ensure that the date state is up-to-date. \"\"\" return_code = pytest . main ([ \"-n\" , \"auto\" , \"--verbose\" ]) raise typer . Exit ( return_code ) amora models list \u00b6 List the models in your project as a human readable table or as a JSON serialized document $ amora models list You can also use the option --with-total-bytes to use BigQuery query dry run feature to gather model total bytes information $ amora models list --with-total-bytes Source code in amora/cli.py @models . command ( name = \"list\" ) def models_list ( format : str = typer . Option ( \"table\" , help = \"Output format. Options: json,table\" , ), with_total_bytes : bool = typer . Option ( False , help = \"Uses BigQuery query dry run feature \" \"to gather model total bytes information\" , ), ) -> None : \"\"\" List the models in your project as a human readable table or as a JSON serialized document ```shell $ amora models list ``` You can also use the option `--with-total-bytes` to use BigQuery query dry run feature to gather model total bytes information ```shell $ amora models list --with-total-bytes ``` \"\"\" @dataclass class ResultItem : model : Model dry_run_result : Optional [ DryRunResult ] = None def as_dict ( self ): return { \"depends_on\" : self . depends_on , \"has_source\" : self . has_source , \"materialization_type\" : self . materialization_type , \"model_name\" : self . model_name , \"referenced_tables\" : self . referenced_tables , \"total_bytes\" : self . total_bytes , } @property def model_name ( self ): return self . model . __name__ @property def has_source ( self ): return self . model . source () is not None @property def depends_on ( self ) -> List [ str ]: return sorted ( [ dependency . __name__ for dependency in self . model . dependencies () ] ) @property def total_bytes ( self ) -> Optional [ int ]: if self . dry_run_result : return self . dry_run_result . total_bytes return None @property def referenced_tables ( self ) -> List [ str ]: if self . dry_run_result : return self . dry_run_result . referenced_tables return [] @property def materialization_type ( self ) -> Optional [ str ]: if self . has_source : return self . model . __model_config__ . materialized . value else : return None results = [] for model , model_file_path in list_models (): if with_total_bytes : result_item = ResultItem ( model = model , dry_run_result = dry_run ( model )) else : result_item = ResultItem ( model = model , dry_run_result = None ) results . append ( result_item ) if format == \"table\" : table = Table ( show_header = True , header_style = \"bold\" , show_lines = True , width = settings . CLI_CONSOLE_MAX_WIDTH , row_styles = [ \"none\" , \"dim\" ], ) table . add_column ( \"Model name\" , style = \"green bold\" , no_wrap = True ) table . add_column ( \"Total bytes\" , no_wrap = True ) table . add_column ( \"Referenced tables\" ) table . add_column ( \"Depends on\" ) table . add_column ( \"Has source?\" , no_wrap = True , justify = \"center\" ) table . add_column ( \"Materialization\" , no_wrap = True ) for result in results : table . add_row ( result . model_name , f \" { result . total_bytes or '-' } \" , Text ( \" \\n \" . join ( result . referenced_tables ) or \"-\" , overflow = \"fold\" ), Text ( \" \\n \" . join ( result . depends_on ) or \"-\" , overflow = \"fold\" ), \"\ud83d\udfe2\" if result . has_source else \"\ud83d\udd34\" , result . materialization_type or \"-\" , ) console = Console ( width = settings . CLI_CONSOLE_MAX_WIDTH ) console . print ( table ) elif format == \"json\" : output = { \"models\" : [ result . as_dict () for result in results ]} typer . echo ( json . dumps ( output )) amora models import \u00b6 Generates a new amora model file from an existing table/view $ amora models import --table-reference my_gcp_project.my_dataset.my_table my_gcp_project/my_dataset/my_table Source code in amora/cli.py @models . command ( name = \"import\" ) def models_import ( table_reference : str = typer . Option ( ... , \"--table-reference\" , help = \"BigQuery unique table identifier. \" \"E.g.: project-id.dataset-id.table-id\" , ), model_file_path : str = typer . Argument ( None , help = \"Canonical name of python module for the generated AmoraModel. \" \"A good pattern would be to use an unique \" \"and deterministic identifier, like: `project_id.dataset_id.table_id`\" , ), overwrite : bool = typer . Option ( False , help = \"Overwrite the output file if one already exists\" ), ): \"\"\" Generates a new amora model file from an existing table/view ```shell $ amora models import --table-reference my_gcp_project.my_dataset.my_table my_gcp_project/my_dataset/my_table ``` \"\"\" env = Environment ( loader = PackageLoader ( \"amora\" ), autoescape = select_autoescape () ) template = env . get_template ( \"new-model.py.jinja2\" ) project , dataset , table = table_reference . split ( \".\" ) model_name = \"\" . join (( part . title () for part in table . split ( \"_\" ))) destination_file_path = Path ( settings . MODELS_PATH ) . joinpath ( ( model_file_path or model_name . replace ( \".\" , \"/\" )) + \".py\" ) if destination_file_path . exists () and not overwrite : typer . echo ( f \"` { destination_file_path } ` already exists. \" f \"Pass `--overwrite` to overwrite file.\" , err = True , ) raise typer . Exit ( 1 ) model_source_code = template . render ( BIGQUERY_TYPES_TO_PYTHON_TYPES = BIGQUERY_TYPES_TO_PYTHON_TYPES , dataset = dataset , dataset_id = f \" { project } . { dataset } \" , model_name = model_name , project = project , schema = get_schema ( table_reference ), table = table , ) destination_file_path . parent . mkdir ( parents = True , exist_ok = True ) destination_file_path . write_text ( model_source_code ) typer . secho ( f \"\ud83c\udf89 Amora Model ` { model_name } ` (` { table_reference } `) imported!\" , fg = typer . colors . GREEN , bold = True , ) typer . secho ( f \"Current File Path: ` { destination_file_path . as_posix () } `\" )","title":"CLI"},{"location":"cli/#cli","text":"","title":"CLI"},{"location":"cli/#amora-compile","text":"Generates executable SQL from model files. Compiled SQL files are written to the ./target directory. Source code in amora/cli.py @app . command () def compile ( models : Optional [ Models ] = models_option , target : Optional [ str ] = target_option , ) -> None : \"\"\" Generates executable SQL from model files. Compiled SQL files are written to the `./target` directory. \"\"\" for model , model_file_path in list_models (): if models and model_file_path . stem not in models : continue source_sql_statement = model . source () if source_sql_statement is None : typer . echo ( f \"\u23ed Skipping compilation of model ` { model_file_path } `\" ) continue target_file_path = model . target_path ( model_file_path ) typer . echo ( f \"\ud83c\udfd7 Compiling model ` { model_file_path } ` -> ` { target_file_path } `\" ) content = compile_statement ( source_sql_statement ) target_file_path . parent . mkdir ( parents = True , exist_ok = True ) target_file_path . write_text ( content )","title":"amora compile"},{"location":"cli/#amora-materialize","text":"Executes the compiled SQL against the current target database. Source code in amora/cli.py @app . command () def materialize ( models : Optional [ Models ] = models_option , target : str = target_option , draw_dag : bool = typer . Option ( False , \"--draw-dag\" ), ) -> None : \"\"\" Executes the compiled SQL against the current target database. \"\"\" model_to_task = {} for target_file_path in list_target_files (): if models and target_file_path . stem not in models : continue task = materialization . Task . for_target ( target_file_path ) model_to_task [ task . model . unique_name ] = task dag = materialization . DependencyDAG . from_tasks ( tasks = model_to_task . values ()) if draw_dag : dag . draw () for model in dag : try : task = model_to_task [ model ] except KeyError : typer . echo ( f \"\u26a0\ufe0f Skipping ` { model } `\" ) continue else : table = materialization . materialize ( sql = task . sql_stmt , model = task . model ) if table is None : continue typer . echo ( f \"\u2705 Created ` { model } ` as ` { table . full_table_id } `\" ) typer . echo ( f \" Rows: { table . num_rows } \" ) typer . echo ( f \" Bytes: { table . num_bytes } \" )","title":"amora materialize"},{"location":"cli/#amora-test","text":"Runs tests on data in deployed models. Run this after amora materialize to ensure that the date state is up-to-date. Source code in amora/cli.py @app . command () def test ( models : Optional [ Models ] = models_option , ) -> None : \"\"\" Runs tests on data in deployed models. Run this after `amora materialize` to ensure that the date state is up-to-date. \"\"\" return_code = pytest . main ([ \"-n\" , \"auto\" , \"--verbose\" ]) raise typer . Exit ( return_code )","title":"amora test"},{"location":"cli/#amora-models-list","text":"List the models in your project as a human readable table or as a JSON serialized document $ amora models list You can also use the option --with-total-bytes to use BigQuery query dry run feature to gather model total bytes information $ amora models list --with-total-bytes Source code in amora/cli.py @models . command ( name = \"list\" ) def models_list ( format : str = typer . Option ( \"table\" , help = \"Output format. Options: json,table\" , ), with_total_bytes : bool = typer . Option ( False , help = \"Uses BigQuery query dry run feature \" \"to gather model total bytes information\" , ), ) -> None : \"\"\" List the models in your project as a human readable table or as a JSON serialized document ```shell $ amora models list ``` You can also use the option `--with-total-bytes` to use BigQuery query dry run feature to gather model total bytes information ```shell $ amora models list --with-total-bytes ``` \"\"\" @dataclass class ResultItem : model : Model dry_run_result : Optional [ DryRunResult ] = None def as_dict ( self ): return { \"depends_on\" : self . depends_on , \"has_source\" : self . has_source , \"materialization_type\" : self . materialization_type , \"model_name\" : self . model_name , \"referenced_tables\" : self . referenced_tables , \"total_bytes\" : self . total_bytes , } @property def model_name ( self ): return self . model . __name__ @property def has_source ( self ): return self . model . source () is not None @property def depends_on ( self ) -> List [ str ]: return sorted ( [ dependency . __name__ for dependency in self . model . dependencies () ] ) @property def total_bytes ( self ) -> Optional [ int ]: if self . dry_run_result : return self . dry_run_result . total_bytes return None @property def referenced_tables ( self ) -> List [ str ]: if self . dry_run_result : return self . dry_run_result . referenced_tables return [] @property def materialization_type ( self ) -> Optional [ str ]: if self . has_source : return self . model . __model_config__ . materialized . value else : return None results = [] for model , model_file_path in list_models (): if with_total_bytes : result_item = ResultItem ( model = model , dry_run_result = dry_run ( model )) else : result_item = ResultItem ( model = model , dry_run_result = None ) results . append ( result_item ) if format == \"table\" : table = Table ( show_header = True , header_style = \"bold\" , show_lines = True , width = settings . CLI_CONSOLE_MAX_WIDTH , row_styles = [ \"none\" , \"dim\" ], ) table . add_column ( \"Model name\" , style = \"green bold\" , no_wrap = True ) table . add_column ( \"Total bytes\" , no_wrap = True ) table . add_column ( \"Referenced tables\" ) table . add_column ( \"Depends on\" ) table . add_column ( \"Has source?\" , no_wrap = True , justify = \"center\" ) table . add_column ( \"Materialization\" , no_wrap = True ) for result in results : table . add_row ( result . model_name , f \" { result . total_bytes or '-' } \" , Text ( \" \\n \" . join ( result . referenced_tables ) or \"-\" , overflow = \"fold\" ), Text ( \" \\n \" . join ( result . depends_on ) or \"-\" , overflow = \"fold\" ), \"\ud83d\udfe2\" if result . has_source else \"\ud83d\udd34\" , result . materialization_type or \"-\" , ) console = Console ( width = settings . CLI_CONSOLE_MAX_WIDTH ) console . print ( table ) elif format == \"json\" : output = { \"models\" : [ result . as_dict () for result in results ]} typer . echo ( json . dumps ( output ))","title":"amora models list"},{"location":"cli/#amora-models-import","text":"Generates a new amora model file from an existing table/view $ amora models import --table-reference my_gcp_project.my_dataset.my_table my_gcp_project/my_dataset/my_table Source code in amora/cli.py @models . command ( name = \"import\" ) def models_import ( table_reference : str = typer . Option ( ... , \"--table-reference\" , help = \"BigQuery unique table identifier. \" \"E.g.: project-id.dataset-id.table-id\" , ), model_file_path : str = typer . Argument ( None , help = \"Canonical name of python module for the generated AmoraModel. \" \"A good pattern would be to use an unique \" \"and deterministic identifier, like: `project_id.dataset_id.table_id`\" , ), overwrite : bool = typer . Option ( False , help = \"Overwrite the output file if one already exists\" ), ): \"\"\" Generates a new amora model file from an existing table/view ```shell $ amora models import --table-reference my_gcp_project.my_dataset.my_table my_gcp_project/my_dataset/my_table ``` \"\"\" env = Environment ( loader = PackageLoader ( \"amora\" ), autoescape = select_autoescape () ) template = env . get_template ( \"new-model.py.jinja2\" ) project , dataset , table = table_reference . split ( \".\" ) model_name = \"\" . join (( part . title () for part in table . split ( \"_\" ))) destination_file_path = Path ( settings . MODELS_PATH ) . joinpath ( ( model_file_path or model_name . replace ( \".\" , \"/\" )) + \".py\" ) if destination_file_path . exists () and not overwrite : typer . echo ( f \"` { destination_file_path } ` already exists. \" f \"Pass `--overwrite` to overwrite file.\" , err = True , ) raise typer . Exit ( 1 ) model_source_code = template . render ( BIGQUERY_TYPES_TO_PYTHON_TYPES = BIGQUERY_TYPES_TO_PYTHON_TYPES , dataset = dataset , dataset_id = f \" { project } . { dataset } \" , model_name = model_name , project = project , schema = get_schema ( table_reference ), table = table , ) destination_file_path . parent . mkdir ( parents = True , exist_ok = True ) destination_file_path . write_text ( model_source_code ) typer . secho ( f \"\ud83c\udf89 Amora Model ` { model_name } ` (` { table_reference } `) imported!\" , fg = typer . colors . GREEN , bold = True , ) typer . secho ( f \"Current File Path: ` { destination_file_path . as_posix () } `\" )","title":"amora models import"},{"location":"examples/","text":"Examples \u00b6 Amora demo project: amora-project","title":"Examples"},{"location":"examples/#examples","text":"Amora demo project: amora-project","title":"Examples"},{"location":"features/","text":"Features \u00b6","title":"Features"},{"location":"features/#features","text":"","title":"Features"},{"location":"models/","text":"Amora Model \u00b6 An Amora Model is a subclass of amora.models.AmoraModel . A way of expressing a data schema , the data materialization and an optional transformation statement. AmoraModel is built on top of SQLModel . Data schema \u00b6 from datetime import datetime from amora.models import AmoraModel , ModelConfig , MaterializationTypes from sqlmodel import Field class Health ( AmoraModel , table = True ): __model_config__ = ModelConfig ( materialized = MaterializationTypes . table , description = \"Health data exported by the Apple Health App\" , ) id : int = Field ( primary_key = True ) type : str sourceName : str sourceVersion : str unit : str creationDate : datetime startDate : datetime endDate : datetime value : float device : str Model Configuration \u00b6 Model configuration metadata Attributes: Name Type Description materialized amora.models.MaterializationTypes The materialization configuration: view , table , ephemeral . Default: view partition_by Optional[PartitionConfig] BigQuery supports the use of a partition by clause to easily partition a table by a column or expression. This option can help decrease latency and cost when querying large tables. cluster_by List[str] BigQuery tables can be clustered to colocate related data. Expects a column of a list of columns. labels Dict[str,str] A dict of labels that can be used as the resource selection description int A string description of the model, used for documentation Transformation \u00b6 Data transformation is defined at the model source() -> Compilable classmethod. Called when amora compile is executed, Amora will build this model in your data warehouse by wrapping it in a create view as or create table as statement. Return None for defining models for tables/views that already exist on the data warehouse and shouldn't be managed by Amora. Return a Compilable , which is a sqlalchemy select statement, in order to compile the model with the given statement :return: Source code in amora/models.py @classmethod def source ( cls ) -> Optional [ Compilable ]: \"\"\" Called when `amora compile` is executed, Amora will build this model in your data warehouse by wrapping it in a `create view as` or `create table as` statement. Return `None` for defining models for tables/views that already exist on the data warehouse and shouldn't be managed by Amora. Return a `Compilable`, which is a sqlalchemy select statement, in order to compile the model with the given statement :return: \"\"\" return None Dependencies \u00b6 A list of Amora Models that the current model depends on Source models \u00b6 Tables/views that already exist on the Data Warehouse and shouldn't be managed by Amora. from datetime import datetime from amora.models import AmoraModel , ModelConfig , MaterializationTypes from sqlmodel import Field class Health ( AmoraModel , table = True ): __model_config__ = ModelConfig ( materialized = MaterializationTypes . table , description = \"Health data exported by the Apple Health App\" , ) id : int = Field ( primary_key = True ) type : str sourceName : str sourceVersion : str unit : str creationDate : datetime startDate : datetime endDate : datetime value : float device : str Source models are models managed outside the scope of Amora, without a source implementation and no dependencies . Model configurations such as materialization type and description are optional, and used for documentation purposes only. Materialized models \u00b6 from amora.compilation import Compilable from amora.models import AmoraModel , MaterializationTypes , ModelConfig from examples.amora_project.models.steps import Steps from sqlmodel import func , select , Field class StepsAgg ( AmoraModel , table = True ): __depends_on__ = [ Steps ] __tablename__ = \"steps_agg\" __model_config__ = ModelConfig ( materialized = MaterializationTypes . table ) avg : float sum : float count : float year : int = Field ( primary_key = True ) month : int = Field ( primary_key = True ) @classmethod def source ( cls ) -> Compilable : sub = select ( [ func . avg ( Steps . value ) . label ( \"avg\" ), func . sum ( Steps . value ) . label ( \"sum\" ), func . count ( Steps . value ) . label ( \"count\" ), func . extract ( \"year\" , Steps . creationDate ) . label ( \"year\" ), func . extract ( \"month\" , Steps . creationDate ) . label ( \"month\" ), ] ) . group_by ( func . extract ( \"year\" , Steps . creationDate ), func . extract ( \"month\" , Steps . creationDate ), ) return sub To-do","title":"Amora Model"},{"location":"models/#amora-model","text":"An Amora Model is a subclass of amora.models.AmoraModel . A way of expressing a data schema , the data materialization and an optional transformation statement. AmoraModel is built on top of SQLModel .","title":"Amora Model"},{"location":"models/#data-schema","text":"from datetime import datetime from amora.models import AmoraModel , ModelConfig , MaterializationTypes from sqlmodel import Field class Health ( AmoraModel , table = True ): __model_config__ = ModelConfig ( materialized = MaterializationTypes . table , description = \"Health data exported by the Apple Health App\" , ) id : int = Field ( primary_key = True ) type : str sourceName : str sourceVersion : str unit : str creationDate : datetime startDate : datetime endDate : datetime value : float device : str","title":"Data schema"},{"location":"models/#model-configuration","text":"Model configuration metadata Attributes: Name Type Description materialized amora.models.MaterializationTypes The materialization configuration: view , table , ephemeral . Default: view partition_by Optional[PartitionConfig] BigQuery supports the use of a partition by clause to easily partition a table by a column or expression. This option can help decrease latency and cost when querying large tables. cluster_by List[str] BigQuery tables can be clustered to colocate related data. Expects a column of a list of columns. labels Dict[str,str] A dict of labels that can be used as the resource selection description int A string description of the model, used for documentation","title":"Model Configuration"},{"location":"models/#transformation","text":"Data transformation is defined at the model source() -> Compilable classmethod. Called when amora compile is executed, Amora will build this model in your data warehouse by wrapping it in a create view as or create table as statement. Return None for defining models for tables/views that already exist on the data warehouse and shouldn't be managed by Amora. Return a Compilable , which is a sqlalchemy select statement, in order to compile the model with the given statement :return: Source code in amora/models.py @classmethod def source ( cls ) -> Optional [ Compilable ]: \"\"\" Called when `amora compile` is executed, Amora will build this model in your data warehouse by wrapping it in a `create view as` or `create table as` statement. Return `None` for defining models for tables/views that already exist on the data warehouse and shouldn't be managed by Amora. Return a `Compilable`, which is a sqlalchemy select statement, in order to compile the model with the given statement :return: \"\"\" return None","title":"Transformation"},{"location":"models/#dependencies","text":"A list of Amora Models that the current model depends on","title":"Dependencies"},{"location":"models/#source-models","text":"Tables/views that already exist on the Data Warehouse and shouldn't be managed by Amora. from datetime import datetime from amora.models import AmoraModel , ModelConfig , MaterializationTypes from sqlmodel import Field class Health ( AmoraModel , table = True ): __model_config__ = ModelConfig ( materialized = MaterializationTypes . table , description = \"Health data exported by the Apple Health App\" , ) id : int = Field ( primary_key = True ) type : str sourceName : str sourceVersion : str unit : str creationDate : datetime startDate : datetime endDate : datetime value : float device : str Source models are models managed outside the scope of Amora, without a source implementation and no dependencies . Model configurations such as materialization type and description are optional, and used for documentation purposes only.","title":"Source models"},{"location":"models/#materialized-models","text":"from amora.compilation import Compilable from amora.models import AmoraModel , MaterializationTypes , ModelConfig from examples.amora_project.models.steps import Steps from sqlmodel import func , select , Field class StepsAgg ( AmoraModel , table = True ): __depends_on__ = [ Steps ] __tablename__ = \"steps_agg\" __model_config__ = ModelConfig ( materialized = MaterializationTypes . table ) avg : float sum : float count : float year : int = Field ( primary_key = True ) month : int = Field ( primary_key = True ) @classmethod def source ( cls ) -> Compilable : sub = select ( [ func . avg ( Steps . value ) . label ( \"avg\" ), func . sum ( Steps . value ) . label ( \"sum\" ), func . count ( Steps . value ) . label ( \"count\" ), func . extract ( \"year\" , Steps . creationDate ) . label ( \"year\" ), func . extract ( \"month\" , Steps . creationDate ) . label ( \"month\" ), ] ) . group_by ( func . extract ( \"year\" , Steps . creationDate ), func . extract ( \"month\" , Steps . creationDate ), ) return sub To-do","title":"Materialized models"}]}