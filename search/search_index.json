{"config":{"indexing":"full","lang":["pt"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Amora Data Build Tool (ADBT) \u00b6 Amora Data Build Tool enables analysts and engineers to transform data on the data warehouse (BigQuery) by writing Amora Models that describe the data schema using Python's PEP484 - Type Hints and select statements with SQLAlchemy . Amora is able to transform Python code into SQL data transformation jobs that run inside the warehouse. When should I use? \u00b6 Use case 1 Use case 2 Use case 3 Installation \u00b6 Pypi \u00b6 pip install amora Setup Big Query permissions \u00b6 In order to use Amora with BigQuery, you'll need to setup a keyfile . Go to the BigQuery credential wizard . Ensure that the right project is selected in the header bar. Generate credentials with the following options: Which API are you using? BigQuery API What data will you be accessing? Application data (you'll be creating a service account) Are you planning to use this API with App Engine or Compute Engine? No Service account name: amora-user Role : BigQuery Job User & BigQuery User Key type : JSON Download the JSON file and save it in an easy-to-remember spot, with a clear filename (e.g. ~/.bq-service-accounts/amora-user-credentials.json ) Set the environment variable GOOGLE_APPLICATION_CREDENTIALS to the path of the JSON file that contains your service account key. You can add export GOOGLE_APPLICATION_CREDENTIALS=~/.bq-service-accounts/amora-user-credentials.json to your shell initialization script ( .zshrc for zsh, .bash_profile for bash, ...) Required roles: BigQuery Data Editor ( roles/bigquery.dataEditor ) BigQuery User ( roles/bigquery.user ) How to use? \u00b6 Amora is packed with a compiler and a runner . The compiler (CLI amora compile ) compiles Amora's python models into SQL statements which can be executed against the configured data warehouse using amora materialize . The animation above displays an user inside an Amora Project using Amora's CLI amora compile to compile model files into SQL statements and creating the corresponding views/tables into the Data Warehouse (BigQuery) using amora materialize . If you want to know more about the features, check the features page","title":"Amora"},{"location":"#amora-data-build-tool-adbt","text":"Amora Data Build Tool enables analysts and engineers to transform data on the data warehouse (BigQuery) by writing Amora Models that describe the data schema using Python's PEP484 - Type Hints and select statements with SQLAlchemy . Amora is able to transform Python code into SQL data transformation jobs that run inside the warehouse.","title":"Amora Data Build Tool (ADBT)"},{"location":"#when-should-i-use","text":"Use case 1 Use case 2 Use case 3","title":"When should I use?"},{"location":"#installation","text":"","title":"Installation"},{"location":"#pypi","text":"pip install amora","title":"Pypi"},{"location":"#setup-big-query-permissions","text":"In order to use Amora with BigQuery, you'll need to setup a keyfile . Go to the BigQuery credential wizard . Ensure that the right project is selected in the header bar. Generate credentials with the following options: Which API are you using? BigQuery API What data will you be accessing? Application data (you'll be creating a service account) Are you planning to use this API with App Engine or Compute Engine? No Service account name: amora-user Role : BigQuery Job User & BigQuery User Key type : JSON Download the JSON file and save it in an easy-to-remember spot, with a clear filename (e.g. ~/.bq-service-accounts/amora-user-credentials.json ) Set the environment variable GOOGLE_APPLICATION_CREDENTIALS to the path of the JSON file that contains your service account key. You can add export GOOGLE_APPLICATION_CREDENTIALS=~/.bq-service-accounts/amora-user-credentials.json to your shell initialization script ( .zshrc for zsh, .bash_profile for bash, ...) Required roles: BigQuery Data Editor ( roles/bigquery.dataEditor ) BigQuery User ( roles/bigquery.user )","title":"Setup Big Query permissions"},{"location":"#how-to-use","text":"Amora is packed with a compiler and a runner . The compiler (CLI amora compile ) compiles Amora's python models into SQL statements which can be executed against the configured data warehouse using amora materialize . The animation above displays an user inside an Amora Project using Amora's CLI amora compile to compile model files into SQL statements and creating the corresponding views/tables into the Data Warehouse (BigQuery) using amora materialize . If you want to know more about the features, check the features page","title":"How to use?"},{"location":"abbreviations/","text":"","title":"Abbreviations"},{"location":"cli/","text":"CLI \u00b6 Amora comes packed with a Command Line Interface, developed using tiangolo/typer . To check all the options, type amora --help after the installation. Usage: amora [OPTIONS] COMMAND [ARGS]... Amora Data Build Tool enables engineers to transform data in their warehouses by defining schemas and writing select statements with SQLAlchemy. Amora handles turning these select statements into tables and views Options: --install-completion Install completion for the current shell. --show-completion Show completion for the current shell, to copy it or customize the installation. --help Show this message and exit. Commands: compile Generates executable SQL from model files. materialize Executes the compiled SQL against the current target... models test Runs tests on data in deployed models. amora compile \u00b6 Generates executable SQL from model files. Compiled SQL files are written to the ./target directory. Source code in amora/cli.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 @app . command () def compile ( models : Optional [ Models ] = models_option , target : Optional [ str ] = target_option , ) -> None : \"\"\" Generates executable SQL from model files. Compiled SQL files are written to the `./target` directory. \"\"\" for model , model_file_path in list_models (): if models and model_file_path . stem not in models : continue source_sql_statement = model . source () if source_sql_statement is None : typer . echo ( f \"\u23ed Skipping compilation of model ` { model_file_path } `\" ) continue target_file_path = model . target_path ( model_file_path ) typer . echo ( f \"\ud83c\udfd7 Compiling model ` { model_file_path } ` -> ` { target_file_path } `\" ) content = compile_statement ( source_sql_statement ) target_file_path . parent . mkdir ( parents = True , exist_ok = True ) target_file_path . write_text ( content ) amora materialize \u00b6 Executes the compiled SQL against the current target database. Source code in amora/cli.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 @app . command () def materialize ( models : Optional [ Models ] = models_option , target : str = target_option , draw_dag : bool = typer . Option ( False , \"--draw-dag\" ), ) -> None : \"\"\" Executes the compiled SQL against the current target database. \"\"\" model_to_task = {} for target_file_path in list_target_files (): if models and target_file_path . stem not in models : continue task = materialization . Task . for_target ( target_file_path ) model_to_task [ task . model . unique_name ] = task dag = materialization . DependencyDAG . from_tasks ( tasks = model_to_task . values ()) if draw_dag : dag . draw () for model in dag : try : task = model_to_task [ model ] except KeyError : typer . echo ( f \"\u26a0\ufe0f Skipping ` { model } `\" ) continue else : table = materialization . materialize ( sql = task . sql_stmt , model = task . model ) if table is None : continue typer . echo ( f \"\u2705 Created ` { model } ` as ` { table . full_table_id } `\" ) typer . echo ( f \" Rows: { table . num_rows } \" ) typer . echo ( f \" Bytes: { table . num_bytes } \" ) amora test \u00b6 Runs tests on data in deployed models. Run this after amora materialize to ensure that the date state is up-to-date. Source code in amora/cli.py 114 115 116 117 118 119 120 121 122 123 @app . command () def test ( models : Optional [ Models ] = models_option , ) -> None : \"\"\" Runs tests on data in deployed models. Run this after `amora materialize` to ensure that the date state is up-to-date. \"\"\" return_code = pytest . main ([ \"-n\" , \"auto\" , \"--verbose\" ]) raise typer . Exit ( return_code ) amora models \u00b6 amora models list \u00b6 List the models in your project as a human readable table or as a JSON serialized document amora models list You can also use the option --with-total-bytes to use BigQuery query dry run feature to gather model total bytes information amora models list --with-total-bytes Source code in amora/cli.py 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 @models . command ( name = \"list\" ) def models_list ( format : str = typer . Option ( \"table\" , help = \"Output format. Options: json,table\" , ), with_total_bytes : bool = typer . Option ( False , help = \"Uses BigQuery query dry run feature \" \"to gather model total bytes information\" , ), ) -> None : \"\"\" List the models in your project as a human readable table or as a JSON serialized document ```shell amora models list ``` You can also use the option `--with-total-bytes` to use BigQuery query dry run feature to gather model total bytes information ```shell amora models list --with-total-bytes ``` \"\"\" @dataclass class ResultItem : model : Model dry_run_result : Optional [ DryRunResult ] = None def as_dict ( self ): return { \"depends_on\" : self . depends_on , \"has_source\" : self . has_source , \"materialization_type\" : self . materialization_type , \"model_name\" : self . model_name , \"referenced_tables\" : self . referenced_tables , \"total_bytes\" : self . total_bytes , \"estimated_query_cost_in_usd\" : self . estimated_query_cost_in_usd , \"estimated_storage_cost_in_usd\" : self . estimated_storage_cost_in_usd , } @property def model_name ( self ): return self . model . __name__ @property def has_source ( self ): return self . model . source () is not None @property def depends_on ( self ) -> List [ str ]: return sorted ( dependency . __name__ for dependency in self . model . dependencies () ) @property def estimated_query_cost_in_usd ( self ) -> Optional [ str ]: if self . dry_run_result : cost = estimated_query_cost_in_usd ( self . dry_run_result . total_bytes ) return f \" { cost : . { settings . MONEY_DECIMAL_PLACES } f } \" return None @property def estimated_storage_cost_in_usd ( self ) -> Optional [ str ]: if self . dry_run_result : cost = estimated_storage_cost_in_usd ( self . dry_run_result . total_bytes ) return f \" { cost : . { settings . MONEY_DECIMAL_PLACES } f } \" return None @property def total_bytes ( self ) -> Optional [ int ]: if self . dry_run_result : return self . dry_run_result . total_bytes return None @property def referenced_tables ( self ) -> List [ str ]: if self . dry_run_result : return self . dry_run_result . referenced_tables return [] @property def materialization_type ( self ) -> Optional [ str ]: if self . has_source : return self . model . __model_config__ . materialized . value else : return None results = [] placeholder = \"-\" for model , model_file_path in list_models (): if with_total_bytes : result_item = ResultItem ( model = model , dry_run_result = dry_run ( model )) else : result_item = ResultItem ( model = model , dry_run_result = None ) results . append ( result_item ) if format == \"table\" : table = Table ( show_header = True , header_style = \"bold\" , show_lines = True , width = settings . CLI_CONSOLE_MAX_WIDTH , row_styles = [ \"none\" , \"dim\" ], ) table . add_column ( \"Model name\" , style = \"green bold\" , no_wrap = True ) table . add_column ( \"Total bytes\" , no_wrap = True ) table . add_column ( \"Estimated query cost\" , no_wrap = True ) table . add_column ( \"Estimated storage cost\" , no_wrap = True ) table . add_column ( \"Referenced tables\" ) table . add_column ( \"Depends on\" ) table . add_column ( \"Has source?\" , no_wrap = True , justify = \"center\" ) table . add_column ( \"Materialization\" , no_wrap = True ) for result in results : table . add_row ( result . model_name , f \" { result . total_bytes or placeholder } \" , result . estimated_query_cost_in_usd or placeholder , result . estimated_storage_cost_in_usd or placeholder , Text ( \" \\n \" . join ( result . referenced_tables ) or placeholder , overflow = \"fold\" , ), Text ( \" \\n \" . join ( result . depends_on ) or placeholder , overflow = \"fold\" ), \"\ud83d\udfe2\" if result . has_source else \"\ud83d\udd34\" , result . materialization_type or placeholder , ) console = Console ( width = settings . CLI_CONSOLE_MAX_WIDTH ) console . print ( table ) elif format == \"json\" : output = { \"models\" : [ result . as_dict () for result in results ]} typer . echo ( json . dumps ( output )) If a machine readable format is required, the --format json option can be used as followed: $ amora models list --format json { \"models\" : [ { \"depends_on\" : [ \"Health\" ], \"has_source\" : true , \"materialization_type\" : \"table\" , \"model_name\" : \"Steps\" , \"referenced_tables\" : [], \"total_bytes\" : null }, { \"depends_on\" : [ \"HeartRate\" ], \"has_source\" : true , \"materialization_type\" : \"table\" , \"model_name\" : \"HeartRateAgg\" , \"referenced_tables\" : [], \"total_bytes\" : null }, { \"depends_on\" : [], \"has_source\" : false , \"materialization_type\" : null , \"model_name\" : \"Health\" , \"referenced_tables\" : [], \"total_bytes\" : null }, { \"depends_on\" : [ \"Health\" ], \"has_source\" : true , \"materialization_type\" : \"table\" , \"model_name\" : \"HeartRate\" , \"referenced_tables\" : [], \"total_bytes\" : null }, { \"depends_on\" : [ \"Steps\" ], \"has_source\" : true , \"materialization_type\" : \"table\" , \"model_name\" : \"StepsAgg\" , \"referenced_tables\" : [], \"total_bytes\" : null } ] } amora models import \u00b6 Generates a new amora model file from an existing table/view amora models import --table-reference my_gcp_project.my_dataset.my_table my_gcp_project/my_dataset/my_table Source code in amora/cli.py 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 @models . command ( name = \"import\" ) def models_import ( table_reference : str = typer . Option ( ... , \"--table-reference\" , help = \"BigQuery unique table identifier. \" \"E.g.: project-id.dataset-id.table-id\" , ), model_file_path : str = typer . Argument ( None , help = \"Canonical name of python module for the generated AmoraModel. \" \"A good pattern would be to use an unique \" \"and deterministic identifier, like: `project_id.dataset_id.table_id`\" , ), overwrite : bool = typer . Option ( False , help = \"Overwrite the output file if one already exists\" ), ): \"\"\" Generates a new amora model file from an existing table/view ```shell amora models import --table-reference my_gcp_project.my_dataset.my_table my_gcp_project/my_dataset/my_table ``` \"\"\" env = Environment ( loader = PackageLoader ( \"amora\" ), autoescape = select_autoescape ()) template = env . get_template ( \"new-model.py.jinja2\" ) project , dataset , table = table_reference . split ( \".\" ) model_name = \"\" . join ( part . title () for part in table . split ( \"_\" )) destination_file_path = Path ( settings . MODELS_PATH ) . joinpath ( ( model_file_path or model_name . replace ( \".\" , \"/\" )) + \".py\" ) if destination_file_path . exists () and not overwrite : typer . echo ( f \"` { destination_file_path } ` already exists. \" f \"Pass `--overwrite` to overwrite file.\" , err = True , ) raise typer . Exit ( 1 ) model_source_code = template . render ( BIGQUERY_TYPES_TO_PYTHON_TYPES = BIGQUERY_TYPES_TO_PYTHON_TYPES , dataset = dataset , dataset_id = f \" { project } . { dataset } \" , model_name = model_name , project = project , schema = get_schema ( table_reference ), table = table , ) destination_file_path . parent . mkdir ( parents = True , exist_ok = True ) destination_file_path . write_text ( model_source_code ) typer . secho ( f \"\ud83c\udf89 Amora Model ` { model_name } ` (` { table_reference } `) imported!\" , fg = typer . colors . GREEN , bold = True , ) typer . secho ( f \"Current File Path: ` { destination_file_path . as_posix () } `\" ) amora feature-store \u00b6 Info Requires the feature-store package extra amora feature-store plan \u00b6 Dry-run registering objects to the Feature Registry The plan method dry-runs registering one or more definitions (e.g.: Entity, Feature View) and produces a list of all the changes that would be introduced in the Feature Registry by an amora feature-store apply execution. The changes computed by the plan command are informational, and are not actually applied to the registry. Source code in amora/cli.py 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 @feature_store . command ( name = \"plan\" ) def feature_store_plan (): \"\"\" Dry-run registering objects to the Feature Registry The plan method dry-runs registering one or more definitions (e.g.: Entity, Feature View) and produces a list of all the changes that would be introduced in the Feature Registry by an `amora feature-store apply` execution. The changes computed by the `plan` command are informational, and are not actually applied to the registry. \"\"\" from amora.feature_store import fs from amora.feature_store.registry import get_repo_contents registry_diff , infra_diff , infra = fs . _plan ( desired_repo_contents = get_repo_contents () ) typer . echo ( \"Amora: Feature Store :: Registry diff\" ) typer . echo ( registry_diff . to_string ()) typer . echo ( \"Amora: Feature Store :: Infrastructure diff\" ) typer . echo ( infra_diff . to_string ()) amora feature-store apply \u00b6 Scans Python files in your amora project and find all models defined as feature views. Validate your feature definitions Sync the metadata about feature store objects to the feature registry. If a registry does not exist, then it will be instantiated. The standard registry is a simple protobuf binary file that is stored on disk (locally or in an object store). Create all necessary feature store infrastructure. The exact infrastructure that is deployed or configured depends on the provider configuration. For example, setting local as your provider will result in a sqlite online store being created. Source code in amora/cli.py 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 @feature_store . command ( name = \"apply\" ) def feature_store_apply (): \"\"\" 1. Scans Python files in your amora project and find all models defined as feature views. 2. Validate your feature definitions 3. Sync the metadata about feature store objects to the feature registry. If a registry does not exist, then it will be instantiated. The standard registry is a simple protobuf binary file that is stored on disk (locally or in an object store). 4. Create all necessary feature store infrastructure. The exact infrastructure that is deployed or configured depends on the provider configuration. For example, setting local as your provider will result in a sqlite online store being created. \"\"\" from feast.repo_operations import apply_total_with_repo_instance from amora.feature_store import fs from amora.feature_store.registry import get_repo_contents apply_total_with_repo_instance ( store = fs , project = fs . project , registry = fs . registry , repo = get_repo_contents (), skip_source_validation = False , ) amora feature-store materialize \u00b6 Run a (non-incremental) materialization job to ingest data into the online store. All data between start_ts and end_ts will be read from the offline store and written into the online store. If you don't specify feature view names using --models , all registered Feature Views will be materialized. Source code in amora/cli.py 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 @feature_store . command ( name = \"materialize\" ) def feature_store_materialize ( start_ts : str = typer . Argument ( None , help = \"Start timestamp on ISO 8601 format. E.g.: '2022-01-01T01:00:00'\" ), end_ts : str = typer . Argument ( None , help = \"End timestamp on ISO 8601 format. E.g.: '2022-01-02T01:00:00'\" ), models : Optional [ Models ] = models_option , ): \"\"\" Run a (non-incremental) materialization job to ingest data into the online store. All data between `start_ts` and `end_ts` will be read from the offline store and written into the online store. If you don't specify feature view names using `--models`, all registered Feature Views will be materialized. \"\"\" from amora.feature_store import fs from amora.feature_store.registry import get_repo_contents repo_contents = get_repo_contents () if models : views_to_materialize = [ fv . name for fv in repo_contents . feature_views if fv . name in models ] else : views_to_materialize = [ fv . name for fv in repo_contents . feature_views ] fs . materialize ( feature_views = views_to_materialize , start_date = datetime . fromisoformat ( start_ts ), end_date = datetime . fromisoformat ( end_ts ), )","title":"CLI"},{"location":"cli/#cli","text":"Amora comes packed with a Command Line Interface, developed using tiangolo/typer . To check all the options, type amora --help after the installation. Usage: amora [OPTIONS] COMMAND [ARGS]... Amora Data Build Tool enables engineers to transform data in their warehouses by defining schemas and writing select statements with SQLAlchemy. Amora handles turning these select statements into tables and views Options: --install-completion Install completion for the current shell. --show-completion Show completion for the current shell, to copy it or customize the installation. --help Show this message and exit. Commands: compile Generates executable SQL from model files. materialize Executes the compiled SQL against the current target... models test Runs tests on data in deployed models.","title":"CLI"},{"location":"cli/#amora-compile","text":"Generates executable SQL from model files. Compiled SQL files are written to the ./target directory. Source code in amora/cli.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 @app . command () def compile ( models : Optional [ Models ] = models_option , target : Optional [ str ] = target_option , ) -> None : \"\"\" Generates executable SQL from model files. Compiled SQL files are written to the `./target` directory. \"\"\" for model , model_file_path in list_models (): if models and model_file_path . stem not in models : continue source_sql_statement = model . source () if source_sql_statement is None : typer . echo ( f \"\u23ed Skipping compilation of model ` { model_file_path } `\" ) continue target_file_path = model . target_path ( model_file_path ) typer . echo ( f \"\ud83c\udfd7 Compiling model ` { model_file_path } ` -> ` { target_file_path } `\" ) content = compile_statement ( source_sql_statement ) target_file_path . parent . mkdir ( parents = True , exist_ok = True ) target_file_path . write_text ( content )","title":"amora compile"},{"location":"cli/#amora-materialize","text":"Executes the compiled SQL against the current target database. Source code in amora/cli.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 @app . command () def materialize ( models : Optional [ Models ] = models_option , target : str = target_option , draw_dag : bool = typer . Option ( False , \"--draw-dag\" ), ) -> None : \"\"\" Executes the compiled SQL against the current target database. \"\"\" model_to_task = {} for target_file_path in list_target_files (): if models and target_file_path . stem not in models : continue task = materialization . Task . for_target ( target_file_path ) model_to_task [ task . model . unique_name ] = task dag = materialization . DependencyDAG . from_tasks ( tasks = model_to_task . values ()) if draw_dag : dag . draw () for model in dag : try : task = model_to_task [ model ] except KeyError : typer . echo ( f \"\u26a0\ufe0f Skipping ` { model } `\" ) continue else : table = materialization . materialize ( sql = task . sql_stmt , model = task . model ) if table is None : continue typer . echo ( f \"\u2705 Created ` { model } ` as ` { table . full_table_id } `\" ) typer . echo ( f \" Rows: { table . num_rows } \" ) typer . echo ( f \" Bytes: { table . num_bytes } \" )","title":"amora materialize"},{"location":"cli/#amora-test","text":"Runs tests on data in deployed models. Run this after amora materialize to ensure that the date state is up-to-date. Source code in amora/cli.py 114 115 116 117 118 119 120 121 122 123 @app . command () def test ( models : Optional [ Models ] = models_option , ) -> None : \"\"\" Runs tests on data in deployed models. Run this after `amora materialize` to ensure that the date state is up-to-date. \"\"\" return_code = pytest . main ([ \"-n\" , \"auto\" , \"--verbose\" ]) raise typer . Exit ( return_code )","title":"amora test"},{"location":"cli/#amora-models","text":"","title":"amora models"},{"location":"cli/#amora-models-list","text":"List the models in your project as a human readable table or as a JSON serialized document amora models list You can also use the option --with-total-bytes to use BigQuery query dry run feature to gather model total bytes information amora models list --with-total-bytes Source code in amora/cli.py 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 @models . command ( name = \"list\" ) def models_list ( format : str = typer . Option ( \"table\" , help = \"Output format. Options: json,table\" , ), with_total_bytes : bool = typer . Option ( False , help = \"Uses BigQuery query dry run feature \" \"to gather model total bytes information\" , ), ) -> None : \"\"\" List the models in your project as a human readable table or as a JSON serialized document ```shell amora models list ``` You can also use the option `--with-total-bytes` to use BigQuery query dry run feature to gather model total bytes information ```shell amora models list --with-total-bytes ``` \"\"\" @dataclass class ResultItem : model : Model dry_run_result : Optional [ DryRunResult ] = None def as_dict ( self ): return { \"depends_on\" : self . depends_on , \"has_source\" : self . has_source , \"materialization_type\" : self . materialization_type , \"model_name\" : self . model_name , \"referenced_tables\" : self . referenced_tables , \"total_bytes\" : self . total_bytes , \"estimated_query_cost_in_usd\" : self . estimated_query_cost_in_usd , \"estimated_storage_cost_in_usd\" : self . estimated_storage_cost_in_usd , } @property def model_name ( self ): return self . model . __name__ @property def has_source ( self ): return self . model . source () is not None @property def depends_on ( self ) -> List [ str ]: return sorted ( dependency . __name__ for dependency in self . model . dependencies () ) @property def estimated_query_cost_in_usd ( self ) -> Optional [ str ]: if self . dry_run_result : cost = estimated_query_cost_in_usd ( self . dry_run_result . total_bytes ) return f \" { cost : . { settings . MONEY_DECIMAL_PLACES } f } \" return None @property def estimated_storage_cost_in_usd ( self ) -> Optional [ str ]: if self . dry_run_result : cost = estimated_storage_cost_in_usd ( self . dry_run_result . total_bytes ) return f \" { cost : . { settings . MONEY_DECIMAL_PLACES } f } \" return None @property def total_bytes ( self ) -> Optional [ int ]: if self . dry_run_result : return self . dry_run_result . total_bytes return None @property def referenced_tables ( self ) -> List [ str ]: if self . dry_run_result : return self . dry_run_result . referenced_tables return [] @property def materialization_type ( self ) -> Optional [ str ]: if self . has_source : return self . model . __model_config__ . materialized . value else : return None results = [] placeholder = \"-\" for model , model_file_path in list_models (): if with_total_bytes : result_item = ResultItem ( model = model , dry_run_result = dry_run ( model )) else : result_item = ResultItem ( model = model , dry_run_result = None ) results . append ( result_item ) if format == \"table\" : table = Table ( show_header = True , header_style = \"bold\" , show_lines = True , width = settings . CLI_CONSOLE_MAX_WIDTH , row_styles = [ \"none\" , \"dim\" ], ) table . add_column ( \"Model name\" , style = \"green bold\" , no_wrap = True ) table . add_column ( \"Total bytes\" , no_wrap = True ) table . add_column ( \"Estimated query cost\" , no_wrap = True ) table . add_column ( \"Estimated storage cost\" , no_wrap = True ) table . add_column ( \"Referenced tables\" ) table . add_column ( \"Depends on\" ) table . add_column ( \"Has source?\" , no_wrap = True , justify = \"center\" ) table . add_column ( \"Materialization\" , no_wrap = True ) for result in results : table . add_row ( result . model_name , f \" { result . total_bytes or placeholder } \" , result . estimated_query_cost_in_usd or placeholder , result . estimated_storage_cost_in_usd or placeholder , Text ( \" \\n \" . join ( result . referenced_tables ) or placeholder , overflow = \"fold\" , ), Text ( \" \\n \" . join ( result . depends_on ) or placeholder , overflow = \"fold\" ), \"\ud83d\udfe2\" if result . has_source else \"\ud83d\udd34\" , result . materialization_type or placeholder , ) console = Console ( width = settings . CLI_CONSOLE_MAX_WIDTH ) console . print ( table ) elif format == \"json\" : output = { \"models\" : [ result . as_dict () for result in results ]} typer . echo ( json . dumps ( output )) If a machine readable format is required, the --format json option can be used as followed: $ amora models list --format json { \"models\" : [ { \"depends_on\" : [ \"Health\" ], \"has_source\" : true , \"materialization_type\" : \"table\" , \"model_name\" : \"Steps\" , \"referenced_tables\" : [], \"total_bytes\" : null }, { \"depends_on\" : [ \"HeartRate\" ], \"has_source\" : true , \"materialization_type\" : \"table\" , \"model_name\" : \"HeartRateAgg\" , \"referenced_tables\" : [], \"total_bytes\" : null }, { \"depends_on\" : [], \"has_source\" : false , \"materialization_type\" : null , \"model_name\" : \"Health\" , \"referenced_tables\" : [], \"total_bytes\" : null }, { \"depends_on\" : [ \"Health\" ], \"has_source\" : true , \"materialization_type\" : \"table\" , \"model_name\" : \"HeartRate\" , \"referenced_tables\" : [], \"total_bytes\" : null }, { \"depends_on\" : [ \"Steps\" ], \"has_source\" : true , \"materialization_type\" : \"table\" , \"model_name\" : \"StepsAgg\" , \"referenced_tables\" : [], \"total_bytes\" : null } ] }","title":"amora models list"},{"location":"cli/#amora-models-import","text":"Generates a new amora model file from an existing table/view amora models import --table-reference my_gcp_project.my_dataset.my_table my_gcp_project/my_dataset/my_table Source code in amora/cli.py 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 @models . command ( name = \"import\" ) def models_import ( table_reference : str = typer . Option ( ... , \"--table-reference\" , help = \"BigQuery unique table identifier. \" \"E.g.: project-id.dataset-id.table-id\" , ), model_file_path : str = typer . Argument ( None , help = \"Canonical name of python module for the generated AmoraModel. \" \"A good pattern would be to use an unique \" \"and deterministic identifier, like: `project_id.dataset_id.table_id`\" , ), overwrite : bool = typer . Option ( False , help = \"Overwrite the output file if one already exists\" ), ): \"\"\" Generates a new amora model file from an existing table/view ```shell amora models import --table-reference my_gcp_project.my_dataset.my_table my_gcp_project/my_dataset/my_table ``` \"\"\" env = Environment ( loader = PackageLoader ( \"amora\" ), autoescape = select_autoescape ()) template = env . get_template ( \"new-model.py.jinja2\" ) project , dataset , table = table_reference . split ( \".\" ) model_name = \"\" . join ( part . title () for part in table . split ( \"_\" )) destination_file_path = Path ( settings . MODELS_PATH ) . joinpath ( ( model_file_path or model_name . replace ( \".\" , \"/\" )) + \".py\" ) if destination_file_path . exists () and not overwrite : typer . echo ( f \"` { destination_file_path } ` already exists. \" f \"Pass `--overwrite` to overwrite file.\" , err = True , ) raise typer . Exit ( 1 ) model_source_code = template . render ( BIGQUERY_TYPES_TO_PYTHON_TYPES = BIGQUERY_TYPES_TO_PYTHON_TYPES , dataset = dataset , dataset_id = f \" { project } . { dataset } \" , model_name = model_name , project = project , schema = get_schema ( table_reference ), table = table , ) destination_file_path . parent . mkdir ( parents = True , exist_ok = True ) destination_file_path . write_text ( model_source_code ) typer . secho ( f \"\ud83c\udf89 Amora Model ` { model_name } ` (` { table_reference } `) imported!\" , fg = typer . colors . GREEN , bold = True , ) typer . secho ( f \"Current File Path: ` { destination_file_path . as_posix () } `\" )","title":"amora models import"},{"location":"cli/#amora-feature-store","text":"Info Requires the feature-store package extra","title":"amora feature-store"},{"location":"cli/#amora-feature-store-plan","text":"Dry-run registering objects to the Feature Registry The plan method dry-runs registering one or more definitions (e.g.: Entity, Feature View) and produces a list of all the changes that would be introduced in the Feature Registry by an amora feature-store apply execution. The changes computed by the plan command are informational, and are not actually applied to the registry. Source code in amora/cli.py 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 @feature_store . command ( name = \"plan\" ) def feature_store_plan (): \"\"\" Dry-run registering objects to the Feature Registry The plan method dry-runs registering one or more definitions (e.g.: Entity, Feature View) and produces a list of all the changes that would be introduced in the Feature Registry by an `amora feature-store apply` execution. The changes computed by the `plan` command are informational, and are not actually applied to the registry. \"\"\" from amora.feature_store import fs from amora.feature_store.registry import get_repo_contents registry_diff , infra_diff , infra = fs . _plan ( desired_repo_contents = get_repo_contents () ) typer . echo ( \"Amora: Feature Store :: Registry diff\" ) typer . echo ( registry_diff . to_string ()) typer . echo ( \"Amora: Feature Store :: Infrastructure diff\" ) typer . echo ( infra_diff . to_string ())","title":"amora feature-store plan"},{"location":"cli/#amora-feature-store-apply","text":"Scans Python files in your amora project and find all models defined as feature views. Validate your feature definitions Sync the metadata about feature store objects to the feature registry. If a registry does not exist, then it will be instantiated. The standard registry is a simple protobuf binary file that is stored on disk (locally or in an object store). Create all necessary feature store infrastructure. The exact infrastructure that is deployed or configured depends on the provider configuration. For example, setting local as your provider will result in a sqlite online store being created. Source code in amora/cli.py 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 @feature_store . command ( name = \"apply\" ) def feature_store_apply (): \"\"\" 1. Scans Python files in your amora project and find all models defined as feature views. 2. Validate your feature definitions 3. Sync the metadata about feature store objects to the feature registry. If a registry does not exist, then it will be instantiated. The standard registry is a simple protobuf binary file that is stored on disk (locally or in an object store). 4. Create all necessary feature store infrastructure. The exact infrastructure that is deployed or configured depends on the provider configuration. For example, setting local as your provider will result in a sqlite online store being created. \"\"\" from feast.repo_operations import apply_total_with_repo_instance from amora.feature_store import fs from amora.feature_store.registry import get_repo_contents apply_total_with_repo_instance ( store = fs , project = fs . project , registry = fs . registry , repo = get_repo_contents (), skip_source_validation = False , )","title":"amora feature-store apply"},{"location":"cli/#amora-feature-store-materialize","text":"Run a (non-incremental) materialization job to ingest data into the online store. All data between start_ts and end_ts will be read from the offline store and written into the online store. If you don't specify feature view names using --models , all registered Feature Views will be materialized. Source code in amora/cli.py 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 @feature_store . command ( name = \"materialize\" ) def feature_store_materialize ( start_ts : str = typer . Argument ( None , help = \"Start timestamp on ISO 8601 format. E.g.: '2022-01-01T01:00:00'\" ), end_ts : str = typer . Argument ( None , help = \"End timestamp on ISO 8601 format. E.g.: '2022-01-02T01:00:00'\" ), models : Optional [ Models ] = models_option , ): \"\"\" Run a (non-incremental) materialization job to ingest data into the online store. All data between `start_ts` and `end_ts` will be read from the offline store and written into the online store. If you don't specify feature view names using `--models`, all registered Feature Views will be materialized. \"\"\" from amora.feature_store import fs from amora.feature_store.registry import get_repo_contents repo_contents = get_repo_contents () if models : views_to_materialize = [ fv . name for fv in repo_contents . feature_views if fv . name in models ] else : views_to_materialize = [ fv . name for fv in repo_contents . feature_views ] fs . materialize ( feature_views = views_to_materialize , start_date = datetime . fromisoformat ( start_ts ), end_date = datetime . fromisoformat ( end_ts ), )","title":"amora feature-store materialize"},{"location":"examples/","text":"Examples \u00b6 Amora demo project: amora-project","title":"\ud83c\udf81 Examples"},{"location":"examples/#examples","text":"Amora demo project: amora-project","title":"Examples"},{"location":"features/","text":"Features \u00b6","title":"Features"},{"location":"features/#features","text":"","title":"Features"},{"location":"glossary/","text":"Glossary \u00b6 Data Model A data model organizes data elements and standardizes how the data elements relate to one another. [1] Entity Entities are used to identify the primary key on which feature values are stored and retrieved. Feature A feature is an individual measurable property. It is typically a property observed on a specific entity, but does not have to be associated with one. For example, a feature of a customer entity could be the number of transactions they have made on an average month, while a feature that is not observed on a specific entity could be the total number of transactions made by all customers in the last month. Training-Serving Skew Training-serving skew is a difference between performance during training and performance during serving. This skew can be caused by: A discrepancy between how you handle data in the training and serving pipelines. A change in the data between when you train and when you serve. A feedback loop between your model and your algorithm. https://cedar.princeton.edu/understanding-data/what-data-model","title":"\ud83d\udd16 Glossary"},{"location":"glossary/#glossary","text":"Data Model A data model organizes data elements and standardizes how the data elements relate to one another. [1] Entity Entities are used to identify the primary key on which feature values are stored and retrieved. Feature A feature is an individual measurable property. It is typically a property observed on a specific entity, but does not have to be associated with one. For example, a feature of a customer entity could be the number of transactions they have made on an average month, while a feature that is not observed on a specific entity could be the total number of transactions made by all customers in the last month. Training-Serving Skew Training-serving skew is a difference between performance during training and performance during serving. This skew can be caused by: A discrepancy between how you handle data in the training and serving pipelines. A change in the data between when you train and when you serve. A feedback loop between your model and your algorithm. https://cedar.princeton.edu/understanding-data/what-data-model","title":"Glossary"},{"location":"concepts/amora-model/","text":"Amora Model \u00b6 An Amora Model is a subclass of amora.models.AmoraModel . A way of expressing a data schema , the data materialization and an optional transformation statement. AmoraModel is built on top of SQLModel . Data schema \u00b6 from datetime import datetime from sqlalchemy import TIMESTAMP from sqlmodel import Field from amora.models import AmoraModel , Column , MaterializationTypes , ModelConfig class Health ( AmoraModel , table = True ): __model_config__ = ModelConfig ( materialized = MaterializationTypes . table , description = \"Health data exported by the Apple Health App\" , ) id : int = Field ( primary_key = True , description = \"Identificador \u00fanico da medida\" ) type : str = Field ( description = \"Tipo da m\u00e9trica coletada\" ) sourceName : str = Field ( description = \"Origem dos dados\" ) sourceVersion : str = Field ( description = \"Vers\u00e3o da origem de dados\" ) unit : str = Field ( description = \"Unidade de medida\" ) value : float = Field ( description = \"Valor observado\" ) device : str = Field ( description = \"Dispositivo de origem dos dados\" ) creationDate : datetime = Field ( description = \"Data de inser\u00e7\u00e3o dos dados\" , sa_column = Column ( TIMESTAMP ) ) startDate : datetime = Field ( description = \"Data do in\u00edcio da medida\" , sa_column = Column ( TIMESTAMP ) ) endDate : datetime = Field ( description = \"Data do fim da medida\" , sa_column = Column ( TIMESTAMP ) ) Model Configuration \u00b6 Model configuration metadata Attributes: Name Type Description cluster_by List [ str ] BigQuery tables can be clustered to colocate related data. Expects a list of columns, as strings. description Optional [ str ] A string description of the model, used for documentation labels Dict [ str , str ] A dict of labels that can be used for resource selection materialized amora . models . MaterializationTypes The materialization configuration: view , table , ephemeral . Default: view partition_by Optional [ PartitionConfig ] BigQuery supports the use of a partition by clause to easily partition a table by a column or expression. This option can help decrease latency and cost when querying large tables. Source code in amora/models.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 @dataclass class ModelConfig : \"\"\" Model configuration metadata Attributes: cluster_by (List[str]): BigQuery tables can be [clustered](https://cloud.google.com/bigquery/docs/clustered-tables) to colocate related data. Expects a list of columns, as strings. description (Optional[str]): A string description of the model, used for documentation labels (Dict[str,str]): A dict of labels that can be used for resource selection materialized (amora.models.MaterializationTypes): The materialization configuration: `view`, `table`, `ephemeral`. Default: `view` partition_by (Optional[PartitionConfig]): BigQuery supports the use of a [partition by](https://cloud.google.com/bigquery/docs/partitioned-tables) clause to easily partition a table by a column or expression. This option can help decrease latency and cost when querying large tables. \"\"\" description : str = \"Undocumented! Generated by Amora Data Build Tool \ud83d\udc9a\" materialized : MaterializationTypes = MaterializationTypes . view partition_by : Optional [ PartitionConfig ] = None cluster_by : List [ str ] = field ( default_factory = list ) labels : Dict [ str , str ] = field ( default_factory = dict ) Transformation \u00b6 Data transformation is defined at the model source() -> Compilable classmethod. Called when amora compile is executed, Amora will build this model in your data warehouse by wrapping it in a create view as or create table as statement. Return None for defining models for tables/views that already exist on the data warehouse and shouldn't be managed by Amora. Return a Compilable , which is a sqlalchemy select statement, in order to compile the model with the given statement :return: Source code in amora/models.py 116 117 118 119 120 121 122 123 124 125 126 127 128 @classmethod def source ( cls ) -> Optional [ Compilable ]: \"\"\" Called when `amora compile` is executed, Amora will build this model in your data warehouse by wrapping it in a `create view as` or `create table as` statement. Return `None` for defining models for tables/views that already exist on the data warehouse and shouldn't be managed by Amora. Return a `Compilable`, which is a sqlalchemy select statement, in order to compile the model with the given statement :return: \"\"\" return None Dependencies \u00b6 A list of Amora Models that the current model depends on Source models \u00b6 Tables/views that already exist on the Data Warehouse and shouldn't be managed by Amora. from datetime import datetime from sqlalchemy import TIMESTAMP from sqlmodel import Field from amora.models import AmoraModel , Column , MaterializationTypes , ModelConfig class Health ( AmoraModel , table = True ): __model_config__ = ModelConfig ( materialized = MaterializationTypes . table , description = \"Health data exported by the Apple Health App\" , ) id : int = Field ( primary_key = True , description = \"Identificador \u00fanico da medida\" ) type : str = Field ( description = \"Tipo da m\u00e9trica coletada\" ) sourceName : str = Field ( description = \"Origem dos dados\" ) sourceVersion : str = Field ( description = \"Vers\u00e3o da origem de dados\" ) unit : str = Field ( description = \"Unidade de medida\" ) value : float = Field ( description = \"Valor observado\" ) device : str = Field ( description = \"Dispositivo de origem dos dados\" ) creationDate : datetime = Field ( description = \"Data de inser\u00e7\u00e3o dos dados\" , sa_column = Column ( TIMESTAMP ) ) startDate : datetime = Field ( description = \"Data do in\u00edcio da medida\" , sa_column = Column ( TIMESTAMP ) ) endDate : datetime = Field ( description = \"Data do fim da medida\" , sa_column = Column ( TIMESTAMP ) ) Source models are models managed outside the scope of Amora, without a source implementation and no dependencies . Model configurations such as materialization type and description are optional, and used for documentation purposes only. Materialized models \u00b6 from sqlmodel import Field , func , select from amora.compilation import Compilable from amora.models import AmoraModel , MaterializationTypes , ModelConfig from examples.amora_project.models.steps import Steps class StepsAgg ( AmoraModel , table = True ): __depends_on__ = [ Steps ] __tablename__ = \"steps_agg\" __model_config__ = ModelConfig ( materialized = MaterializationTypes . table ) avg : float sum : float count : int year : int = Field ( primary_key = True ) month : int = Field ( primary_key = True ) @classmethod def source ( cls ) -> Compilable : sub = select ( [ func . avg ( Steps . value ) . label ( \"avg\" ), func . sum ( Steps . value ) . label ( \"sum\" ), func . count ( Steps . value ) . label ( \"count\" ), func . extract ( \"year\" , Steps . creationDate ) . label ( \"year\" ), func . extract ( \"month\" , Steps . creationDate ) . label ( \"month\" ), ] ) . group_by ( func . extract ( \"year\" , Steps . creationDate ), func . extract ( \"month\" , Steps . creationDate ), ) return sub To-do","title":"Amora Model"},{"location":"concepts/amora-model/#amora-model","text":"An Amora Model is a subclass of amora.models.AmoraModel . A way of expressing a data schema , the data materialization and an optional transformation statement. AmoraModel is built on top of SQLModel .","title":"Amora Model"},{"location":"concepts/amora-model/#data-schema","text":"from datetime import datetime from sqlalchemy import TIMESTAMP from sqlmodel import Field from amora.models import AmoraModel , Column , MaterializationTypes , ModelConfig class Health ( AmoraModel , table = True ): __model_config__ = ModelConfig ( materialized = MaterializationTypes . table , description = \"Health data exported by the Apple Health App\" , ) id : int = Field ( primary_key = True , description = \"Identificador \u00fanico da medida\" ) type : str = Field ( description = \"Tipo da m\u00e9trica coletada\" ) sourceName : str = Field ( description = \"Origem dos dados\" ) sourceVersion : str = Field ( description = \"Vers\u00e3o da origem de dados\" ) unit : str = Field ( description = \"Unidade de medida\" ) value : float = Field ( description = \"Valor observado\" ) device : str = Field ( description = \"Dispositivo de origem dos dados\" ) creationDate : datetime = Field ( description = \"Data de inser\u00e7\u00e3o dos dados\" , sa_column = Column ( TIMESTAMP ) ) startDate : datetime = Field ( description = \"Data do in\u00edcio da medida\" , sa_column = Column ( TIMESTAMP ) ) endDate : datetime = Field ( description = \"Data do fim da medida\" , sa_column = Column ( TIMESTAMP ) )","title":"Data schema"},{"location":"concepts/amora-model/#model-configuration","text":"Model configuration metadata Attributes: Name Type Description cluster_by List [ str ] BigQuery tables can be clustered to colocate related data. Expects a list of columns, as strings. description Optional [ str ] A string description of the model, used for documentation labels Dict [ str , str ] A dict of labels that can be used for resource selection materialized amora . models . MaterializationTypes The materialization configuration: view , table , ephemeral . Default: view partition_by Optional [ PartitionConfig ] BigQuery supports the use of a partition by clause to easily partition a table by a column or expression. This option can help decrease latency and cost when querying large tables. Source code in amora/models.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 @dataclass class ModelConfig : \"\"\" Model configuration metadata Attributes: cluster_by (List[str]): BigQuery tables can be [clustered](https://cloud.google.com/bigquery/docs/clustered-tables) to colocate related data. Expects a list of columns, as strings. description (Optional[str]): A string description of the model, used for documentation labels (Dict[str,str]): A dict of labels that can be used for resource selection materialized (amora.models.MaterializationTypes): The materialization configuration: `view`, `table`, `ephemeral`. Default: `view` partition_by (Optional[PartitionConfig]): BigQuery supports the use of a [partition by](https://cloud.google.com/bigquery/docs/partitioned-tables) clause to easily partition a table by a column or expression. This option can help decrease latency and cost when querying large tables. \"\"\" description : str = \"Undocumented! Generated by Amora Data Build Tool \ud83d\udc9a\" materialized : MaterializationTypes = MaterializationTypes . view partition_by : Optional [ PartitionConfig ] = None cluster_by : List [ str ] = field ( default_factory = list ) labels : Dict [ str , str ] = field ( default_factory = dict )","title":"Model Configuration"},{"location":"concepts/amora-model/#transformation","text":"Data transformation is defined at the model source() -> Compilable classmethod. Called when amora compile is executed, Amora will build this model in your data warehouse by wrapping it in a create view as or create table as statement. Return None for defining models for tables/views that already exist on the data warehouse and shouldn't be managed by Amora. Return a Compilable , which is a sqlalchemy select statement, in order to compile the model with the given statement :return: Source code in amora/models.py 116 117 118 119 120 121 122 123 124 125 126 127 128 @classmethod def source ( cls ) -> Optional [ Compilable ]: \"\"\" Called when `amora compile` is executed, Amora will build this model in your data warehouse by wrapping it in a `create view as` or `create table as` statement. Return `None` for defining models for tables/views that already exist on the data warehouse and shouldn't be managed by Amora. Return a `Compilable`, which is a sqlalchemy select statement, in order to compile the model with the given statement :return: \"\"\" return None","title":"Transformation"},{"location":"concepts/amora-model/#dependencies","text":"A list of Amora Models that the current model depends on","title":"Dependencies"},{"location":"concepts/amora-model/#source-models","text":"Tables/views that already exist on the Data Warehouse and shouldn't be managed by Amora. from datetime import datetime from sqlalchemy import TIMESTAMP from sqlmodel import Field from amora.models import AmoraModel , Column , MaterializationTypes , ModelConfig class Health ( AmoraModel , table = True ): __model_config__ = ModelConfig ( materialized = MaterializationTypes . table , description = \"Health data exported by the Apple Health App\" , ) id : int = Field ( primary_key = True , description = \"Identificador \u00fanico da medida\" ) type : str = Field ( description = \"Tipo da m\u00e9trica coletada\" ) sourceName : str = Field ( description = \"Origem dos dados\" ) sourceVersion : str = Field ( description = \"Vers\u00e3o da origem de dados\" ) unit : str = Field ( description = \"Unidade de medida\" ) value : float = Field ( description = \"Valor observado\" ) device : str = Field ( description = \"Dispositivo de origem dos dados\" ) creationDate : datetime = Field ( description = \"Data de inser\u00e7\u00e3o dos dados\" , sa_column = Column ( TIMESTAMP ) ) startDate : datetime = Field ( description = \"Data do in\u00edcio da medida\" , sa_column = Column ( TIMESTAMP ) ) endDate : datetime = Field ( description = \"Data do fim da medida\" , sa_column = Column ( TIMESTAMP ) ) Source models are models managed outside the scope of Amora, without a source implementation and no dependencies . Model configurations such as materialization type and description are optional, and used for documentation purposes only.","title":"Source models"},{"location":"concepts/amora-model/#materialized-models","text":"from sqlmodel import Field , func , select from amora.compilation import Compilable from amora.models import AmoraModel , MaterializationTypes , ModelConfig from examples.amora_project.models.steps import Steps class StepsAgg ( AmoraModel , table = True ): __depends_on__ = [ Steps ] __tablename__ = \"steps_agg\" __model_config__ = ModelConfig ( materialized = MaterializationTypes . table ) avg : float sum : float count : int year : int = Field ( primary_key = True ) month : int = Field ( primary_key = True ) @classmethod def source ( cls ) -> Compilable : sub = select ( [ func . avg ( Steps . value ) . label ( \"avg\" ), func . sum ( Steps . value ) . label ( \"sum\" ), func . count ( Steps . value ) . label ( \"count\" ), func . extract ( \"year\" , Steps . creationDate ) . label ( \"year\" ), func . extract ( \"month\" , Steps . creationDate ) . label ( \"month\" ), ] ) . group_by ( func . extract ( \"year\" , Steps . creationDate ), func . extract ( \"month\" , Steps . creationDate ), ) return sub To-do","title":"Materialized models"},{"location":"feature-store/feature-store/","text":"Feature Store \u00b6 In order to use Amora's feature store, you need to install it with the feature-store extra: pip install amora [ feature-store ] Using Amora's Feature Store capabilities enables data teams to: Run the necessary data pipeline to transform the data into feature values Easily productionize new features from Amora Models Store and manage feature data Track feature lineage, versions and related metadata Serve feature data consistently for training and inference purposes Share and reuse features across multiple teams Components \u00b6 Amora's Feature Store is composed by the following components: graph LR GCS[GCS] subgraph Feature Store subgraph Storage OnS[Online Storage] OffS[Offline Storage] end subgraph Serving O[Feature Service] end subgraph Registry FR[Feature Registry] end end OnS --> R[Redis]; OffS --> B[Big Query]; O -->|get_online_features| M[Model Serving] O -->|get_historical_features| X[Model Training] FR --> GCS Storage \u00b6 Feature data is stored by the Feature Store to support retrieval through Online and Offline feature serving layers. Offline Storage is typically used to store months or years of feature data for training purposes. In Amora, data is stored in Big Query. Online Storage is used to persist feature values for low-latency lookup during inference. It only store the latest feature values for each entity , essentially modeling the current state of the world. In Amora, data is stored in Redis. graph LR subgraph Feature Store subgraph Storage OnS[Online Storage] OffS[Offline Storage] end subgraph Serving O[Feature Service] end end OnS --> R[Redis]; OffS --> B[Big Query]; Serving \u00b6 A ML Model require a consistent view of features through training and serving. The definitions of features used to train a model must exactly match the features provided in online serving. When they don\u2019t match, training-serving skew is introduced, which can cause model performance problems. Amora's Feature Store is able to consistently serve feature data to ML Models: During the generation of training datasets, querying the offline storage for historical feature values. Low-latency retrieval of the latest feature value from the online store. graph LR subgraph Feature Store subgraph Storage OnS[Online Storage] OffS[Offline Storage] end subgraph Serving O[Feature Service] end end O -->|get_online_features| M[Model Serving] O -->|get_historical_features| X[Model Training] Registry \u00b6 The registry is the single source of truth for information and metadata about features in a project. It is a central catalog for Data Teams and automated jobs that registers what kind of data is stored and how it is organized. In Amora, the registry is stored on GCS using Feast's Feature Registry . graph LR GCS[GCS] subgraph Feature Store subgraph Storage OnS[Online Storage] OffS[Offline Storage] end subgraph Serving O[Feature Service] end subgraph Registry FR[Feature Registry] end end FR --> GCS","title":"Feature Store"},{"location":"feature-store/feature-store/#feature-store","text":"In order to use Amora's feature store, you need to install it with the feature-store extra: pip install amora [ feature-store ] Using Amora's Feature Store capabilities enables data teams to: Run the necessary data pipeline to transform the data into feature values Easily productionize new features from Amora Models Store and manage feature data Track feature lineage, versions and related metadata Serve feature data consistently for training and inference purposes Share and reuse features across multiple teams","title":"Feature Store"},{"location":"feature-store/feature-store/#components","text":"Amora's Feature Store is composed by the following components: graph LR GCS[GCS] subgraph Feature Store subgraph Storage OnS[Online Storage] OffS[Offline Storage] end subgraph Serving O[Feature Service] end subgraph Registry FR[Feature Registry] end end OnS --> R[Redis]; OffS --> B[Big Query]; O -->|get_online_features| M[Model Serving] O -->|get_historical_features| X[Model Training] FR --> GCS","title":"Components"},{"location":"feature-store/feature-store/#storage","text":"Feature data is stored by the Feature Store to support retrieval through Online and Offline feature serving layers. Offline Storage is typically used to store months or years of feature data for training purposes. In Amora, data is stored in Big Query. Online Storage is used to persist feature values for low-latency lookup during inference. It only store the latest feature values for each entity , essentially modeling the current state of the world. In Amora, data is stored in Redis. graph LR subgraph Feature Store subgraph Storage OnS[Online Storage] OffS[Offline Storage] end subgraph Serving O[Feature Service] end end OnS --> R[Redis]; OffS --> B[Big Query];","title":"Storage"},{"location":"feature-store/feature-store/#serving","text":"A ML Model require a consistent view of features through training and serving. The definitions of features used to train a model must exactly match the features provided in online serving. When they don\u2019t match, training-serving skew is introduced, which can cause model performance problems. Amora's Feature Store is able to consistently serve feature data to ML Models: During the generation of training datasets, querying the offline storage for historical feature values. Low-latency retrieval of the latest feature value from the online store. graph LR subgraph Feature Store subgraph Storage OnS[Online Storage] OffS[Offline Storage] end subgraph Serving O[Feature Service] end end O -->|get_online_features| M[Model Serving] O -->|get_historical_features| X[Model Training]","title":"Serving"},{"location":"feature-store/feature-store/#registry","text":"The registry is the single source of truth for information and metadata about features in a project. It is a central catalog for Data Teams and automated jobs that registers what kind of data is stored and how it is organized. In Amora, the registry is stored on GCS using Feast's Feature Registry . graph LR GCS[GCS] subgraph Feature Store subgraph Storage OnS[Online Storage] OffS[Offline Storage] end subgraph Serving O[Feature Service] end subgraph Registry FR[Feature Registry] end end FR --> GCS","title":"Registry"},{"location":"feature-store/feature-view-protocol/","text":"Bases: Protocol The contract needed to expose the AmoraModel definition as a Feature View To expose an AmoraModel as a Feature View, one must decorate it with amora.feature_store.decorators.feature_view and implement the FeatureViewProcotol class methods. Hint An example can be found at examples.amora_project.models.step_count_by_source.py Source code in amora/feature_store/protocols.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 @runtime_checkable class FeatureViewSourceProtocol ( Protocol ): \"\"\" The contract needed to expose the [AmoraModel](amora-model) definition as a [Feature View](feature-view.md) To expose an `AmoraModel` as a Feature View, one must decorate it with `amora.feature_store.decorators.feature_view` and implement the `FeatureViewProcotol` class methods. !!! hint An example can be found at `examples.amora_project.models.step_count_by_source.py` \"\"\" @classmethod def feature_view_entities ( cls ) -> List [ Column ]: \"\"\" Returns a list of the model columns that should be used as an [entity](feature-view.md#entities). E.g: An `AmoraModel` with a single `customer_id` entity: ```python @classmethod def feature_view_entities(cls) -> List[Column]: return [cls.customer_id] ``` An `AmoraModel` with multiple entities, `customer_id` and `company_id`: ```python @classmethod def feature_view_entities(cls) -> List[Column]: return [cls.customer_id, cls.company_id] ``` A model with no entities: ```python @classmethod def feature_view_entities(cls) -> List[Column]: return [] ``` \"\"\" ... @classmethod def feature_view_features ( cls ) -> List [ Column ]: \"\"\" Returns a list of the model columns that should be used as [features](feature-view.md#features). E.g: Features of a customer entity could be _the number of transactions they have made on a month_ as `count_transactions_last_30d` and _the sum of the transactions amounts they have made on a month_ as `sum_transactions_last_30d`: ```python @classmethod def feature_view_features(cls) -> List[Column]: return [cls.count_transactions_last_30d, cls.sum_transactions_last_30d] ``` \"\"\" ... @classmethod def feature_view_event_timestamp ( cls ) -> Column : \"\"\" Event timestamp column used for point-in-time joins of feature values. E.g: ```python @classmethod def feature_view_event_timestamp(cls) -> Column: return cls.event_timestamp ``` At your Amora Model, the column should be defined as such: ```python from sqlalchemy import TIMESTAMP, Column from amora.models import AmoraModel, Field @feature_view class FeatureViewModel(AmoraModel, table=True): ... event_timestamp: datetime = Field(sa_column=Column(TIMESTAMP)) ``` \"\"\" ... feature_view_entities () -> List [ Column ] classmethod \u00b6 Returns a list of the model columns that should be used as an entity . E.g: An AmoraModel with a single customer_id entity: @classmethod def feature_view_entities ( cls ) -> List [ Column ]: return [ cls . customer_id ] An AmoraModel with multiple entities, customer_id and company_id : @classmethod def feature_view_entities ( cls ) -> List [ Column ]: return [ cls . customer_id , cls . company_id ] A model with no entities: @classmethod def feature_view_entities ( cls ) -> List [ Column ]: return [] Source code in amora/feature_store/protocols.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 @classmethod def feature_view_entities ( cls ) -> List [ Column ]: \"\"\" Returns a list of the model columns that should be used as an [entity](feature-view.md#entities). E.g: An `AmoraModel` with a single `customer_id` entity: ```python @classmethod def feature_view_entities(cls) -> List[Column]: return [cls.customer_id] ``` An `AmoraModel` with multiple entities, `customer_id` and `company_id`: ```python @classmethod def feature_view_entities(cls) -> List[Column]: return [cls.customer_id, cls.company_id] ``` A model with no entities: ```python @classmethod def feature_view_entities(cls) -> List[Column]: return [] ``` \"\"\" ... feature_view_event_timestamp () -> Column classmethod \u00b6 Event timestamp column used for point-in-time joins of feature values. E.g: @classmethod def feature_view_event_timestamp ( cls ) -> Column : return cls . event_timestamp At your Amora Model, the column should be defined as such: from sqlalchemy import TIMESTAMP , Column from amora.models import AmoraModel , Field @feature_view class FeatureViewModel ( AmoraModel , table = True ): ... event_timestamp : datetime = Field ( sa_column = Column ( TIMESTAMP )) Source code in amora/feature_store/protocols.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 @classmethod def feature_view_event_timestamp ( cls ) -> Column : \"\"\" Event timestamp column used for point-in-time joins of feature values. E.g: ```python @classmethod def feature_view_event_timestamp(cls) -> Column: return cls.event_timestamp ``` At your Amora Model, the column should be defined as such: ```python from sqlalchemy import TIMESTAMP, Column from amora.models import AmoraModel, Field @feature_view class FeatureViewModel(AmoraModel, table=True): ... event_timestamp: datetime = Field(sa_column=Column(TIMESTAMP)) ``` \"\"\" ... feature_view_features () -> List [ Column ] classmethod \u00b6 Returns a list of the model columns that should be used as features . E.g: Features of a customer entity could be the number of transactions they have made on a month as count_transactions_last_30d and the sum of the transactions amounts they have made on a month as sum_transactions_last_30d : @classmethod def feature_view_features ( cls ) -> List [ Column ]: return [ cls . count_transactions_last_30d , cls . sum_transactions_last_30d ] Source code in amora/feature_store/protocols.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 @classmethod def feature_view_features ( cls ) -> List [ Column ]: \"\"\" Returns a list of the model columns that should be used as [features](feature-view.md#features). E.g: Features of a customer entity could be _the number of transactions they have made on a month_ as `count_transactions_last_30d` and _the sum of the transactions amounts they have made on a month_ as `sum_transactions_last_30d`: ```python @classmethod def feature_view_features(cls) -> List[Column]: return [cls.count_transactions_last_30d, cls.sum_transactions_last_30d] ``` \"\"\" ...","title":"Feature View Protocol"},{"location":"feature-store/feature-view-protocol/#amora.feature_store.protocols.FeatureViewSourceProtocol.feature_view_entities","text":"Returns a list of the model columns that should be used as an entity . E.g: An AmoraModel with a single customer_id entity: @classmethod def feature_view_entities ( cls ) -> List [ Column ]: return [ cls . customer_id ] An AmoraModel with multiple entities, customer_id and company_id : @classmethod def feature_view_entities ( cls ) -> List [ Column ]: return [ cls . customer_id , cls . company_id ] A model with no entities: @classmethod def feature_view_entities ( cls ) -> List [ Column ]: return [] Source code in amora/feature_store/protocols.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 @classmethod def feature_view_entities ( cls ) -> List [ Column ]: \"\"\" Returns a list of the model columns that should be used as an [entity](feature-view.md#entities). E.g: An `AmoraModel` with a single `customer_id` entity: ```python @classmethod def feature_view_entities(cls) -> List[Column]: return [cls.customer_id] ``` An `AmoraModel` with multiple entities, `customer_id` and `company_id`: ```python @classmethod def feature_view_entities(cls) -> List[Column]: return [cls.customer_id, cls.company_id] ``` A model with no entities: ```python @classmethod def feature_view_entities(cls) -> List[Column]: return [] ``` \"\"\" ...","title":"feature_view_entities()"},{"location":"feature-store/feature-view-protocol/#amora.feature_store.protocols.FeatureViewSourceProtocol.feature_view_event_timestamp","text":"Event timestamp column used for point-in-time joins of feature values. E.g: @classmethod def feature_view_event_timestamp ( cls ) -> Column : return cls . event_timestamp At your Amora Model, the column should be defined as such: from sqlalchemy import TIMESTAMP , Column from amora.models import AmoraModel , Field @feature_view class FeatureViewModel ( AmoraModel , table = True ): ... event_timestamp : datetime = Field ( sa_column = Column ( TIMESTAMP )) Source code in amora/feature_store/protocols.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 @classmethod def feature_view_event_timestamp ( cls ) -> Column : \"\"\" Event timestamp column used for point-in-time joins of feature values. E.g: ```python @classmethod def feature_view_event_timestamp(cls) -> Column: return cls.event_timestamp ``` At your Amora Model, the column should be defined as such: ```python from sqlalchemy import TIMESTAMP, Column from amora.models import AmoraModel, Field @feature_view class FeatureViewModel(AmoraModel, table=True): ... event_timestamp: datetime = Field(sa_column=Column(TIMESTAMP)) ``` \"\"\" ...","title":"feature_view_event_timestamp()"},{"location":"feature-store/feature-view-protocol/#amora.feature_store.protocols.FeatureViewSourceProtocol.feature_view_features","text":"Returns a list of the model columns that should be used as features . E.g: Features of a customer entity could be the number of transactions they have made on a month as count_transactions_last_30d and the sum of the transactions amounts they have made on a month as sum_transactions_last_30d : @classmethod def feature_view_features ( cls ) -> List [ Column ]: return [ cls . count_transactions_last_30d , cls . sum_transactions_last_30d ] Source code in amora/feature_store/protocols.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 @classmethod def feature_view_features ( cls ) -> List [ Column ]: \"\"\" Returns a list of the model columns that should be used as [features](feature-view.md#features). E.g: Features of a customer entity could be _the number of transactions they have made on a month_ as `count_transactions_last_30d` and _the sum of the transactions amounts they have made on a month_ as `sum_transactions_last_30d`: ```python @classmethod def feature_view_features(cls) -> List[Column]: return [cls.count_transactions_last_30d, cls.sum_transactions_last_30d] ``` \"\"\" ...","title":"feature_view_features()"},{"location":"feature-store/feature-view/","text":"Feature View \u00b6 Entities \u00b6 Entities are used to identify the primary key on which feature values are stored and retrieved. They are used as keys during the lookup of feature values from the Online Store and the join process in point-in-time joins. Entities are generally recognizable, concrete concepts or abstract, such as a customer email, a document number or a surrogate key . It is possible to define multiple entities for a single a feature view and is also possible to have zero entities. A dataset with a customer_id entity : event_timestamp customer_id sum_amount_spent_last_30d 2022-04-04T01:00:00 1 1000 2022-04-04T01:00:00 2 5500 2022-04-04T01:00:00 3 2000 2022-04-05T01:00:00 1 2000 A dataset with multiple entities, customer_id and company_id : event_timestamp customer_id company_id sum_amount_spent_last_30d 2022-04-04T01:00:00 1 a 400 2022-04-04T01:00:00 1 b 600 2022-04-04T01:00:00 2 a 5000 2022-04-04T01:00:00 2 b 500 2022-04-04T01:00:00 3 a 2000 2022-04-05T01:00:00 1 a 2000 Features \u00b6 A feature is an individual measurable property. It is typically a property observed on a specific entity , but does not have to be associated with an entity . A feature of a customer entity could be the number of transactions they have made on a month, count_transactions_last_30d : event_timestamp customer_id count_transactions_last_30d 2022-04-01T01:00:00 1 10 2022-04-01T01:00:00 2 3 2022-04-01T01:00:00 3 5 2022-05-01T01:00:00 1 10 2022-05-01T01:00:00 2 20 2022-05-01T01:00:00 3 30 A feature unrelated to an entity could be the number of transactions made by all customers in the last month, count_all_transactions_last_30d : event_timestamp count_all_transactions_last_30d 2022-04-01T01:00:00 18 2022-05-01T01:00:00 60 Feature View \u00b6 A Feature View aggregates entities, features and a data source, allowing the Feature Store to consistently manage feature data across time. Info Read more on Feast's documentation . On Amora, defining a Feature View from an AmoraModel is done by decorating the model with amora.feature_store.decorators.feature_view and implementing the protocol FeatureViewProtocol . E.g: StepCountBySource is a data model that exposes the features value_avg , value_sum and value_count of each source_name entity on a given event_timestamp . from datetime import datetime from typing import Optional from sqlalchemy import TIMESTAMP , Column , func from amora.feature_store.decorators import feature_view from amora.models import AmoraModel , Field , MaterializationTypes , ModelConfig , select from amora.transformations import datetime_trunc_hour from amora.types import Compilable from examples.amora_project.models.steps import Steps @feature_view class StepCountBySource ( AmoraModel , table = True ): __depends_on__ = [ Steps ] __model_config__ = ModelConfig ( materialized = MaterializationTypes . table ) value_avg : float = Field ( description = \"Average step count of the hour\" ) value_sum : float = Field ( description = \"Sum of the step counts of the hour\" ) value_count : float = Field ( description = \"Count of step count samples of the hour\" ) source_name : str = Field ( primary_key = True , description = \"Source of the metric\" ) event_timestamp : datetime = Field ( primary_key = True , sa_column = Column ( TIMESTAMP )) @classmethod def source ( cls ) -> Optional [ Compilable ]: datetime_trunc = func . timestamp ( datetime_trunc_hour ( Steps . creationDate )) return select ( [ func . avg ( Steps . value ) . label ( cls . value_avg . key ), func . sum ( Steps . value ) . label ( cls . value_sum . key ), func . count ( Steps . value ) . label ( cls . value_count . key ), Steps . sourceName . label ( cls . source_name . key ), datetime_trunc . label ( cls . event_timestamp . key ), ] ) . group_by ( cls . source_name . key , cls . event_timestamp . key ) @classmethod def feature_view_entities ( cls ): return [ cls . source_name ] @classmethod def feature_view_features ( cls ): return [ cls . value_avg , cls . value_sum , cls . value_count , ] @classmethod def feature_view_event_timestamp ( cls ): return cls . event_timestamp","title":"Feature View"},{"location":"feature-store/feature-view/#feature-view","text":"","title":"Feature View"},{"location":"feature-store/feature-view/#entities","text":"Entities are used to identify the primary key on which feature values are stored and retrieved. They are used as keys during the lookup of feature values from the Online Store and the join process in point-in-time joins. Entities are generally recognizable, concrete concepts or abstract, such as a customer email, a document number or a surrogate key . It is possible to define multiple entities for a single a feature view and is also possible to have zero entities. A dataset with a customer_id entity : event_timestamp customer_id sum_amount_spent_last_30d 2022-04-04T01:00:00 1 1000 2022-04-04T01:00:00 2 5500 2022-04-04T01:00:00 3 2000 2022-04-05T01:00:00 1 2000 A dataset with multiple entities, customer_id and company_id : event_timestamp customer_id company_id sum_amount_spent_last_30d 2022-04-04T01:00:00 1 a 400 2022-04-04T01:00:00 1 b 600 2022-04-04T01:00:00 2 a 5000 2022-04-04T01:00:00 2 b 500 2022-04-04T01:00:00 3 a 2000 2022-04-05T01:00:00 1 a 2000","title":"Entities"},{"location":"feature-store/feature-view/#features","text":"A feature is an individual measurable property. It is typically a property observed on a specific entity , but does not have to be associated with an entity . A feature of a customer entity could be the number of transactions they have made on a month, count_transactions_last_30d : event_timestamp customer_id count_transactions_last_30d 2022-04-01T01:00:00 1 10 2022-04-01T01:00:00 2 3 2022-04-01T01:00:00 3 5 2022-05-01T01:00:00 1 10 2022-05-01T01:00:00 2 20 2022-05-01T01:00:00 3 30 A feature unrelated to an entity could be the number of transactions made by all customers in the last month, count_all_transactions_last_30d : event_timestamp count_all_transactions_last_30d 2022-04-01T01:00:00 18 2022-05-01T01:00:00 60","title":"Features"},{"location":"feature-store/feature-view/#feature-view_1","text":"A Feature View aggregates entities, features and a data source, allowing the Feature Store to consistently manage feature data across time. Info Read more on Feast's documentation . On Amora, defining a Feature View from an AmoraModel is done by decorating the model with amora.feature_store.decorators.feature_view and implementing the protocol FeatureViewProtocol . E.g: StepCountBySource is a data model that exposes the features value_avg , value_sum and value_count of each source_name entity on a given event_timestamp . from datetime import datetime from typing import Optional from sqlalchemy import TIMESTAMP , Column , func from amora.feature_store.decorators import feature_view from amora.models import AmoraModel , Field , MaterializationTypes , ModelConfig , select from amora.transformations import datetime_trunc_hour from amora.types import Compilable from examples.amora_project.models.steps import Steps @feature_view class StepCountBySource ( AmoraModel , table = True ): __depends_on__ = [ Steps ] __model_config__ = ModelConfig ( materialized = MaterializationTypes . table ) value_avg : float = Field ( description = \"Average step count of the hour\" ) value_sum : float = Field ( description = \"Sum of the step counts of the hour\" ) value_count : float = Field ( description = \"Count of step count samples of the hour\" ) source_name : str = Field ( primary_key = True , description = \"Source of the metric\" ) event_timestamp : datetime = Field ( primary_key = True , sa_column = Column ( TIMESTAMP )) @classmethod def source ( cls ) -> Optional [ Compilable ]: datetime_trunc = func . timestamp ( datetime_trunc_hour ( Steps . creationDate )) return select ( [ func . avg ( Steps . value ) . label ( cls . value_avg . key ), func . sum ( Steps . value ) . label ( cls . value_sum . key ), func . count ( Steps . value ) . label ( cls . value_count . key ), Steps . sourceName . label ( cls . source_name . key ), datetime_trunc . label ( cls . event_timestamp . key ), ] ) . group_by ( cls . source_name . key , cls . event_timestamp . key ) @classmethod def feature_view_entities ( cls ): return [ cls . source_name ] @classmethod def feature_view_features ( cls ): return [ cls . value_avg , cls . value_sum , cls . value_count , ] @classmethod def feature_view_event_timestamp ( cls ): return cls . event_timestamp","title":"Feature View"},{"location":"tests/assertions/","text":"Data Assertions \u00b6 expression_is_true \u00b6 _test ( statement , raise_on_fail = True ) \u00b6 :param statement: A str with a valid SQL compiled statement :param raise_on_fail: By default, the test will raise a pytest Fail exception, with a debug message. Default True . :return: True if the test passed, False otherwise Source code in amora/tests/assertions.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def _test ( statement : Compilable , raise_on_fail : bool = True ) -> bool : \"\"\" :param statement: A str with a valid SQL compiled statement :param raise_on_fail: By default, the test will raise a pytest Fail exception, with a debug message. Default `True`. :return: `True` if the test passed, `False` otherwise \"\"\" run_result = run ( statement ) _log_result ( run_result ) if run_result . rows . total_rows == 0 : return True elif raise_on_fail : pytest . fail ( f \" { run_result . rows . total_rows } rows failed the test assertion.\" f \" \\n ===========\" f \" \\n Test query:\" f \" \\n ===========\" f \" \\n { run_result . query } \" , pytrace = False , ) else : return False are_unique_together ( columns ) \u00b6 This test confirms that the combination of columns is unique. For example, the combination of month and product is unique, however neither column is unique in isolation. Example: are_unique_together ([ HeartRateAgg . year , HeartRateAgg . month ]) Source code in amora/tests/assertions.py 385 386 387 388 389 390 391 392 393 394 395 396 397 398 def are_unique_together ( columns : Iterable [ ColumnElement ]) -> Compilable : \"\"\" This test confirms that the combination of columns is unique. For example, the combination of month and product is unique, however neither column is unique in isolation. Example: ```python are_unique_together([HeartRateAgg.year, HeartRateAgg.month]) ``` \"\"\" return select ( columns ) . group_by ( * columns ) . having ( func . count ( type_ = Integer ) > 1 ) equality ( model_a , model_b , compare_columns = None ) \u00b6 This schema test asserts the equality of two models. Optionally specify a subset of columns to compare. Source code in amora/tests/assertions.py 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 def equality ( model_a : AmoraModel , model_b : AmoraModel , compare_columns : Optional [ Iterable [ ColumnElement ]] = None , ) -> bool : \"\"\" This schema test asserts the equality of two models. Optionally specify a subset of columns to compare. \"\"\" raise NotImplementedError def comparable_columns ( model : AmoraModel ) -> Iterable [ ColumnElement ]: if not compare_columns : return model return [ getattr ( model , column_name ) for column_name in compare_columns ] a = select ( comparable_columns ( model_a )) . cte ( \"a\" ) b = select ( comparable_columns ( model_b )) . cte ( \"b\" ) # fixme: google.api_core.exceptions.BadRequest: 400 EXCEPT must be followed by ALL, DISTINCT, or \"(\" at [34:4] a_minus_b = select ( a ) . except_ ( select ( b )) b_minus_a = select ( b ) . except_ ( select ( a )) diff_union = union_all ( a_minus_b , b_minus_a ) return _test ( statement = diff_union ) expression_is_true ( expression , condition = None ) \u00b6 Asserts that a expression is TRUE for all records. This is useful when checking integrity across columns, for example, that a total is equal to the sum of its parts, or that at least one column is true. Optionally assert expression only for rows where condition is met. Parameters: Name Type Description Default condition object A query filter None Example: expression_is_true ( StepsAgg . _sum > StepsAgg . _avg , condition = StepsAgg . year == 2021 ) Source code in amora/tests/assertions.py 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 def expression_is_true ( expression , condition = None ) -> bool : \"\"\" Asserts that a expression is TRUE for all records. This is useful when checking integrity across columns, for example, that a total is equal to the sum of its parts, or that at least one column is true. Optionally assert `expression` only for rows where `condition` is met. Arguments: condition (object): A query filter Example: ```python expression_is_true(StepsAgg._sum > StepsAgg._avg, condition=StepsAgg.year == 2021) ``` \"\"\" statement = select ([ \"*\" ]) . where ( ~ expression ) if condition is not None : statement = statement . where ( condition ) return _test ( statement ) has_accepted_values ( column , values ) \u00b6 Assert that the values from the column should be one of the provided values Example SQL: SELECT {{ column_name }} FROM {{ model }} WHERE {{ column_name }} NOT IN {{ values }} Example: has_accepted_values ( HeartRate . source , values = [ \"iPhone\" , \"Mi Band\" ]) Source code in amora/tests/assertions.py 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 def has_accepted_values ( column : ColumnElement , values : Iterable ) -> Compilable : \"\"\" Assert that the values from the `column` should be one of the provided `values` Example SQL: ```sql SELECT {{ column_name }} FROM {{ model }} WHERE {{ column_name }} NOT IN {{ values }} ``` Example: ```python has_accepted_values(HeartRate.source, values=[\"iPhone\", \"Mi Band\"]) ``` \"\"\" return select ( column ) . where ( ~ column . in_ ( values )) has_at_least_one_not_null_value ( column ) \u00b6 Asserts if column has at least one value. Example SQL: SELECT count ( {{ column_name }} ) as filler_column FROM {{ model }} HAVING count ( {{ column_name }} ) = 0 Example: has_at_least_one_not_null_value ( Health . value ) Source code in amora/tests/assertions.py 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 def has_at_least_one_not_null_value ( column : ColumnElement ) -> Compilable : \"\"\" Asserts if column has at least one value. Example SQL: ```sql SELECT count({{ column_name }}) as filler_column FROM {{ model }} HAVING count({{ column_name }}) = 0 ``` Example: ```python has_at_least_one_not_null_value(Health.value) ``` \"\"\" return select ( func . count ( column , type_ = Integer )) . having ( func . count ( column ) == 0 ) is_a_non_empty_string ( column ) \u00b6 Asserts that the column isn't an empty string Example SQL: SELECT {{ column_name }} FROM {{ model }} WHERE TRIM ( {{ column_name }} ) = \"\" Example: is_a_non_empty_string ( Health . source ) Source code in amora/tests/assertions.py 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 def is_a_non_empty_string ( column : ColumnElement ) -> Compilable : \"\"\" Asserts that the column isn't an empty string Example SQL: ```sql SELECT {{ column_name }} FROM {{ model }} WHERE TRIM({{ column_name }}) = \"\" ``` Example: ```python is_a_non_empty_string(Health.source) ``` \"\"\" return select ( column ) . where ( func . trim ( column ) == literal ( \"\" )) is_non_negative ( column ) \u00b6 Asserts that every column value should be >= 0 Example SQL: SELECT {{ column_name }} FROM {{ model }} WHERE {{ column_name }} < 0 Example: is_non_negative ( HeartRate . value ) Source code in amora/tests/assertions.py 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 def is_non_negative ( column : ColumnElement ) -> Compilable : \"\"\" Asserts that every column value should be >= 0 Example SQL: ```sql SELECT {{ column_name }} FROM {{ model }} WHERE {{ column_name }} < 0 ``` Example: ```python is_non_negative(HeartRate.value) ``` \"\"\" return select ( column ) . where ( column < 0 ) is_not_null ( column ) \u00b6 Asserts that the column does not contain null values Results in the following query: SELECT {{ column_name }} FROM {{ model }} WHERE {{ column_name }} IS NULL Example: is_not_null ( HeartRate . id ) Source code in amora/tests/assertions.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def is_not_null ( column : ColumnElement ) -> Compilable : \"\"\" Asserts that the `column` does not contain `null` values Results in the following query: ```sql SELECT {{ column_name }} FROM {{ model }} WHERE {{ column_name }} IS NULL ``` Example: ```python is_not_null(HeartRate.id) ``` \"\"\" return select ( column ) . where ( column == None ) is_numeric ( column ) \u00b6 Asserts that each not null value is a number Example SQL: SELECT {{ column }} FROM {{ model }} WHERE REGEXP_CONTAINS ( {{ column }} , \"[^0-9]\" ) Example: is_numeric ( func . cast ( Health . value , String ) . label ( \"value_as_str\" )) Source code in amora/tests/assertions.py 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 def is_numeric ( column : ColumnElement ) -> Compilable : \"\"\" Asserts that each not null value is a number Example SQL: ```sql SELECT {{ column }} FROM {{ model }} WHERE REGEXP_CONTAINS({{ column }}, \"[^0-9]\") ``` Example: ```python is_numeric(func.cast(Health.value, String).label(\"value_as_str\")) ``` \"\"\" return select ( column ) . where ( func . REGEXP_CONTAINS ( column , \"[^0-9]\" )) is_unique ( column ) \u00b6 Assert that the column values are unique Example SQL: SELECT {{ column_name }} FROM ( SELECT {{ column_name }} FROM {{ model }} WHERE {{ column_name }} IS NOT NULL GROUP BY {{ column_name }} HAVING COUNT ( * ) > 1 ) validation_errors Example: is_unique ( HeartRate . id ) Source code in amora/tests/assertions.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 def is_unique ( column : ColumnElement ) -> Compilable : \"\"\" Assert that the `column` values are unique Example SQL: ```sql SELECT {{ column_name }} FROM ( SELECT {{ column_name }} FROM {{ model }} WHERE {{ column_name }} IS NOT NULL GROUP BY {{ column_name }} HAVING COUNT(*) > 1 ) validation_errors ``` Example: ```python is_unique(HeartRate.id) ``` \"\"\" return select ( column ) . group_by ( column ) . having ( func . count ( column ) > 1 ) relationship ( from_ , to , from_condition = None , to_condition = None ) \u00b6 Each value of the from_ column exists as a value in the to column. Also known as referential integrity. This test validates the referential integrity between two relations with a predicate ( from_condition and to_condition ) to filter out some rows from the test. This is useful to exclude records such as test entities, rows created in the last X minutes/hours to account for temporary gaps due to data ingestion limitations, etc. Example SQL: WITH left_table AS ( SELECT {{ from_column_name }} AS id FROM {{ from_table }} WHERE {{ from_column_name }} IS NOT NULL AND {{ from_condition }} ), right_table AS ( SELECT {{ to_column_name }} AS id FROM {{ to_table }} WHERE {{ to_column_name }} IS NOT NULL AND {{ to_condition }} ), exceptions as ( SELECT left_table . id AS {{ from_column_name }}} FROM left_table LEFT JOIN right_table ON left_table . id = right_table . id WHERE right_table . id IS NULL ) SELECT * FROM exceptions Example: relationship ( HeartRate . id , to = Health . id ) Source code in amora/tests/assertions.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 def relationship ( from_ : ColumnElement , to : ColumnElement , from_condition = None , to_condition = None , ) -> bool : \"\"\" Each value of the `from_` column exists as a value in the `to` column. Also known as referential integrity. This test validates the referential integrity between two relations with a predicate (`from_condition` and `to_condition`) to filter out some rows from the test. This is useful to exclude records such as test entities, rows created in the last X minutes/hours to account for temporary gaps due to data ingestion limitations, etc. Example SQL: ```sql WITH left_table AS ( SELECT {{from_column_name}} AS id FROM {{from_table}} WHERE {{from_column_name}} IS NOT NULL AND {{from_condition}} ), right_table AS ( SELECT {{to_column_name}} AS id FROM {{to_table}} WHERE {{to_column_name}} IS NOT NULL AND {{to_condition}} ), exceptions as ( SELECT left_table.id AS {{from_column_name}}} FROM left_table LEFT JOIN right_table ON left_table.id = right_table.id WHERE right_table.id IS NULL ) SELECT * FROM exceptions ``` Example: ```python relationship(HeartRate.id, to=Health.id) ``` \"\"\" left_table = ( select ( from_ . label ( \"id\" )) . where ( from_ != None ) . where ( from_condition or and_ ( True )) . cte ( \"left_table\" ) ) right_table = ( select ( to . label ( \"id\" )) . where ( to != None ) . where ( to_condition or and_ ( True )) . cte ( \"right_table\" ) ) exceptions = ( select ([ left_table . c [ \"id\" ] . label ( from_ . key )]) . select_from ( left_table . join ( right_table , onclause = left_table . c [ \"id\" ] == right_table . c [ \"id\" ], isouter = True , ) ) . where ( right_table . c [ \"id\" ] == None ) ) return _test ( statement = exceptions ) that ( column , test , raise_on_fail = True , ** test_kwargs ) \u00b6 Executes the test, returning True if the test is successful and raising a pytest fail otherwise Example: assert that ( HeartRate . value , is_not_null ) :param column: An AmoraModel column to test :param test: The test assertion function :param raise_on_fail: By default, the test will raise a pytest Fail exception, with a debug message. Default True . :param test_kwargs: Keyword arguments passed to the test function Source code in amora/tests/assertions.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def that ( column : ColumnElement , test : Test , raise_on_fail : bool = True , ** test_kwargs , ) -> bool : \"\"\" Executes the test, returning `True` if the test is successful and raising a pytest fail otherwise Example: ```python assert that(HeartRate.value, is_not_null) ``` :param column: An AmoraModel column to test :param test: The test assertion function :param raise_on_fail: By default, the test will raise a pytest Fail exception, with a debug message. Default `True`. :param test_kwargs: Keyword arguments passed to the `test` function \"\"\" return _test ( statement = test ( column , ** test_kwargs ), raise_on_fail = raise_on_fail )","title":"Data Assertions"},{"location":"tests/assertions/#data-assertions","text":"","title":"Data Assertions"},{"location":"tests/assertions/#expression_is_true","text":"","title":"expression_is_true"},{"location":"tests/assertions/#amora.tests.assertions._test","text":":param statement: A str with a valid SQL compiled statement :param raise_on_fail: By default, the test will raise a pytest Fail exception, with a debug message. Default True . :return: True if the test passed, False otherwise Source code in amora/tests/assertions.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def _test ( statement : Compilable , raise_on_fail : bool = True ) -> bool : \"\"\" :param statement: A str with a valid SQL compiled statement :param raise_on_fail: By default, the test will raise a pytest Fail exception, with a debug message. Default `True`. :return: `True` if the test passed, `False` otherwise \"\"\" run_result = run ( statement ) _log_result ( run_result ) if run_result . rows . total_rows == 0 : return True elif raise_on_fail : pytest . fail ( f \" { run_result . rows . total_rows } rows failed the test assertion.\" f \" \\n ===========\" f \" \\n Test query:\" f \" \\n ===========\" f \" \\n { run_result . query } \" , pytrace = False , ) else : return False","title":"_test()"},{"location":"tests/assertions/#amora.tests.assertions.are_unique_together","text":"This test confirms that the combination of columns is unique. For example, the combination of month and product is unique, however neither column is unique in isolation. Example: are_unique_together ([ HeartRateAgg . year , HeartRateAgg . month ]) Source code in amora/tests/assertions.py 385 386 387 388 389 390 391 392 393 394 395 396 397 398 def are_unique_together ( columns : Iterable [ ColumnElement ]) -> Compilable : \"\"\" This test confirms that the combination of columns is unique. For example, the combination of month and product is unique, however neither column is unique in isolation. Example: ```python are_unique_together([HeartRateAgg.year, HeartRateAgg.month]) ``` \"\"\" return select ( columns ) . group_by ( * columns ) . having ( func . count ( type_ = Integer ) > 1 )","title":"are_unique_together()"},{"location":"tests/assertions/#amora.tests.assertions.equality","text":"This schema test asserts the equality of two models. Optionally specify a subset of columns to compare. Source code in amora/tests/assertions.py 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 def equality ( model_a : AmoraModel , model_b : AmoraModel , compare_columns : Optional [ Iterable [ ColumnElement ]] = None , ) -> bool : \"\"\" This schema test asserts the equality of two models. Optionally specify a subset of columns to compare. \"\"\" raise NotImplementedError def comparable_columns ( model : AmoraModel ) -> Iterable [ ColumnElement ]: if not compare_columns : return model return [ getattr ( model , column_name ) for column_name in compare_columns ] a = select ( comparable_columns ( model_a )) . cte ( \"a\" ) b = select ( comparable_columns ( model_b )) . cte ( \"b\" ) # fixme: google.api_core.exceptions.BadRequest: 400 EXCEPT must be followed by ALL, DISTINCT, or \"(\" at [34:4] a_minus_b = select ( a ) . except_ ( select ( b )) b_minus_a = select ( b ) . except_ ( select ( a )) diff_union = union_all ( a_minus_b , b_minus_a ) return _test ( statement = diff_union )","title":"equality()"},{"location":"tests/assertions/#amora.tests.assertions.expression_is_true","text":"Asserts that a expression is TRUE for all records. This is useful when checking integrity across columns, for example, that a total is equal to the sum of its parts, or that at least one column is true. Optionally assert expression only for rows where condition is met. Parameters: Name Type Description Default condition object A query filter None Example: expression_is_true ( StepsAgg . _sum > StepsAgg . _avg , condition = StepsAgg . year == 2021 ) Source code in amora/tests/assertions.py 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 def expression_is_true ( expression , condition = None ) -> bool : \"\"\" Asserts that a expression is TRUE for all records. This is useful when checking integrity across columns, for example, that a total is equal to the sum of its parts, or that at least one column is true. Optionally assert `expression` only for rows where `condition` is met. Arguments: condition (object): A query filter Example: ```python expression_is_true(StepsAgg._sum > StepsAgg._avg, condition=StepsAgg.year == 2021) ``` \"\"\" statement = select ([ \"*\" ]) . where ( ~ expression ) if condition is not None : statement = statement . where ( condition ) return _test ( statement )","title":"expression_is_true()"},{"location":"tests/assertions/#amora.tests.assertions.has_accepted_values","text":"Assert that the values from the column should be one of the provided values Example SQL: SELECT {{ column_name }} FROM {{ model }} WHERE {{ column_name }} NOT IN {{ values }} Example: has_accepted_values ( HeartRate . source , values = [ \"iPhone\" , \"Mi Band\" ]) Source code in amora/tests/assertions.py 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 def has_accepted_values ( column : ColumnElement , values : Iterable ) -> Compilable : \"\"\" Assert that the values from the `column` should be one of the provided `values` Example SQL: ```sql SELECT {{ column_name }} FROM {{ model }} WHERE {{ column_name }} NOT IN {{ values }} ``` Example: ```python has_accepted_values(HeartRate.source, values=[\"iPhone\", \"Mi Band\"]) ``` \"\"\" return select ( column ) . where ( ~ column . in_ ( values ))","title":"has_accepted_values()"},{"location":"tests/assertions/#amora.tests.assertions.has_at_least_one_not_null_value","text":"Asserts if column has at least one value. Example SQL: SELECT count ( {{ column_name }} ) as filler_column FROM {{ model }} HAVING count ( {{ column_name }} ) = 0 Example: has_at_least_one_not_null_value ( Health . value ) Source code in amora/tests/assertions.py 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 def has_at_least_one_not_null_value ( column : ColumnElement ) -> Compilable : \"\"\" Asserts if column has at least one value. Example SQL: ```sql SELECT count({{ column_name }}) as filler_column FROM {{ model }} HAVING count({{ column_name }}) = 0 ``` Example: ```python has_at_least_one_not_null_value(Health.value) ``` \"\"\" return select ( func . count ( column , type_ = Integer )) . having ( func . count ( column ) == 0 )","title":"has_at_least_one_not_null_value()"},{"location":"tests/assertions/#amora.tests.assertions.is_a_non_empty_string","text":"Asserts that the column isn't an empty string Example SQL: SELECT {{ column_name }} FROM {{ model }} WHERE TRIM ( {{ column_name }} ) = \"\" Example: is_a_non_empty_string ( Health . source ) Source code in amora/tests/assertions.py 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 def is_a_non_empty_string ( column : ColumnElement ) -> Compilable : \"\"\" Asserts that the column isn't an empty string Example SQL: ```sql SELECT {{ column_name }} FROM {{ model }} WHERE TRIM({{ column_name }}) = \"\" ``` Example: ```python is_a_non_empty_string(Health.source) ``` \"\"\" return select ( column ) . where ( func . trim ( column ) == literal ( \"\" ))","title":"is_a_non_empty_string()"},{"location":"tests/assertions/#amora.tests.assertions.is_non_negative","text":"Asserts that every column value should be >= 0 Example SQL: SELECT {{ column_name }} FROM {{ model }} WHERE {{ column_name }} < 0 Example: is_non_negative ( HeartRate . value ) Source code in amora/tests/assertions.py 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 def is_non_negative ( column : ColumnElement ) -> Compilable : \"\"\" Asserts that every column value should be >= 0 Example SQL: ```sql SELECT {{ column_name }} FROM {{ model }} WHERE {{ column_name }} < 0 ``` Example: ```python is_non_negative(HeartRate.value) ``` \"\"\" return select ( column ) . where ( column < 0 )","title":"is_non_negative()"},{"location":"tests/assertions/#amora.tests.assertions.is_not_null","text":"Asserts that the column does not contain null values Results in the following query: SELECT {{ column_name }} FROM {{ model }} WHERE {{ column_name }} IS NULL Example: is_not_null ( HeartRate . id ) Source code in amora/tests/assertions.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def is_not_null ( column : ColumnElement ) -> Compilable : \"\"\" Asserts that the `column` does not contain `null` values Results in the following query: ```sql SELECT {{ column_name }} FROM {{ model }} WHERE {{ column_name }} IS NULL ``` Example: ```python is_not_null(HeartRate.id) ``` \"\"\" return select ( column ) . where ( column == None )","title":"is_not_null()"},{"location":"tests/assertions/#amora.tests.assertions.is_numeric","text":"Asserts that each not null value is a number Example SQL: SELECT {{ column }} FROM {{ model }} WHERE REGEXP_CONTAINS ( {{ column }} , \"[^0-9]\" ) Example: is_numeric ( func . cast ( Health . value , String ) . label ( \"value_as_str\" )) Source code in amora/tests/assertions.py 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 def is_numeric ( column : ColumnElement ) -> Compilable : \"\"\" Asserts that each not null value is a number Example SQL: ```sql SELECT {{ column }} FROM {{ model }} WHERE REGEXP_CONTAINS({{ column }}, \"[^0-9]\") ``` Example: ```python is_numeric(func.cast(Health.value, String).label(\"value_as_str\")) ``` \"\"\" return select ( column ) . where ( func . REGEXP_CONTAINS ( column , \"[^0-9]\" ))","title":"is_numeric()"},{"location":"tests/assertions/#amora.tests.assertions.is_unique","text":"Assert that the column values are unique Example SQL: SELECT {{ column_name }} FROM ( SELECT {{ column_name }} FROM {{ model }} WHERE {{ column_name }} IS NOT NULL GROUP BY {{ column_name }} HAVING COUNT ( * ) > 1 ) validation_errors Example: is_unique ( HeartRate . id ) Source code in amora/tests/assertions.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 def is_unique ( column : ColumnElement ) -> Compilable : \"\"\" Assert that the `column` values are unique Example SQL: ```sql SELECT {{ column_name }} FROM ( SELECT {{ column_name }} FROM {{ model }} WHERE {{ column_name }} IS NOT NULL GROUP BY {{ column_name }} HAVING COUNT(*) > 1 ) validation_errors ``` Example: ```python is_unique(HeartRate.id) ``` \"\"\" return select ( column ) . group_by ( column ) . having ( func . count ( column ) > 1 )","title":"is_unique()"},{"location":"tests/assertions/#amora.tests.assertions.relationship","text":"Each value of the from_ column exists as a value in the to column. Also known as referential integrity. This test validates the referential integrity between two relations with a predicate ( from_condition and to_condition ) to filter out some rows from the test. This is useful to exclude records such as test entities, rows created in the last X minutes/hours to account for temporary gaps due to data ingestion limitations, etc. Example SQL: WITH left_table AS ( SELECT {{ from_column_name }} AS id FROM {{ from_table }} WHERE {{ from_column_name }} IS NOT NULL AND {{ from_condition }} ), right_table AS ( SELECT {{ to_column_name }} AS id FROM {{ to_table }} WHERE {{ to_column_name }} IS NOT NULL AND {{ to_condition }} ), exceptions as ( SELECT left_table . id AS {{ from_column_name }}} FROM left_table LEFT JOIN right_table ON left_table . id = right_table . id WHERE right_table . id IS NULL ) SELECT * FROM exceptions Example: relationship ( HeartRate . id , to = Health . id ) Source code in amora/tests/assertions.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 def relationship ( from_ : ColumnElement , to : ColumnElement , from_condition = None , to_condition = None , ) -> bool : \"\"\" Each value of the `from_` column exists as a value in the `to` column. Also known as referential integrity. This test validates the referential integrity between two relations with a predicate (`from_condition` and `to_condition`) to filter out some rows from the test. This is useful to exclude records such as test entities, rows created in the last X minutes/hours to account for temporary gaps due to data ingestion limitations, etc. Example SQL: ```sql WITH left_table AS ( SELECT {{from_column_name}} AS id FROM {{from_table}} WHERE {{from_column_name}} IS NOT NULL AND {{from_condition}} ), right_table AS ( SELECT {{to_column_name}} AS id FROM {{to_table}} WHERE {{to_column_name}} IS NOT NULL AND {{to_condition}} ), exceptions as ( SELECT left_table.id AS {{from_column_name}}} FROM left_table LEFT JOIN right_table ON left_table.id = right_table.id WHERE right_table.id IS NULL ) SELECT * FROM exceptions ``` Example: ```python relationship(HeartRate.id, to=Health.id) ``` \"\"\" left_table = ( select ( from_ . label ( \"id\" )) . where ( from_ != None ) . where ( from_condition or and_ ( True )) . cte ( \"left_table\" ) ) right_table = ( select ( to . label ( \"id\" )) . where ( to != None ) . where ( to_condition or and_ ( True )) . cte ( \"right_table\" ) ) exceptions = ( select ([ left_table . c [ \"id\" ] . label ( from_ . key )]) . select_from ( left_table . join ( right_table , onclause = left_table . c [ \"id\" ] == right_table . c [ \"id\" ], isouter = True , ) ) . where ( right_table . c [ \"id\" ] == None ) ) return _test ( statement = exceptions )","title":"relationship()"},{"location":"tests/assertions/#amora.tests.assertions.that","text":"Executes the test, returning True if the test is successful and raising a pytest fail otherwise Example: assert that ( HeartRate . value , is_not_null ) :param column: An AmoraModel column to test :param test: The test assertion function :param raise_on_fail: By default, the test will raise a pytest Fail exception, with a debug message. Default True . :param test_kwargs: Keyword arguments passed to the test function Source code in amora/tests/assertions.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def that ( column : ColumnElement , test : Test , raise_on_fail : bool = True , ** test_kwargs , ) -> bool : \"\"\" Executes the test, returning `True` if the test is successful and raising a pytest fail otherwise Example: ```python assert that(HeartRate.value, is_not_null) ``` :param column: An AmoraModel column to test :param test: The test assertion function :param raise_on_fail: By default, the test will raise a pytest Fail exception, with a debug message. Default `True`. :param test_kwargs: Keyword arguments passed to the `test` function \"\"\" return _test ( statement = test ( column , ** test_kwargs ), raise_on_fail = raise_on_fail )","title":"that()"}]}